<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>论文阅读：Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection | 且离且安的碎碎念</title><meta name="author" content="lian"><meta name="copyright" content="lian"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Introduction 研究动机与问题定义  1）开放世界需求（Why now?）作者把“能处理开放世界场景”的能力视为通往通用人工智能（AGI）的关键指标之一。传统检测器只会固定类别（如 COCO 的 80 类），一旦出现“未标过的新概念”，模型就失灵。这就是所谓**闭集检测（closed-set）**的局限【p.2, Fig.1a】。  2）目标任务（What?）论文要解决的是开放集目标检测">
<meta property="og:type" content="article">
<meta property="og:title" content="论文阅读：Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection">
<meta property="og:url" content="https://qieliqiean.github.io/blog/2025/10/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AGrounding-DINO-Marrying-DINO-with-Grounded-Pre-Training-for-Open-Set-Object-Detection/index.html">
<meta property="og:site_name" content="且离且安的碎碎念">
<meta property="og:description" content="Introduction 研究动机与问题定义  1）开放世界需求（Why now?）作者把“能处理开放世界场景”的能力视为通往通用人工智能（AGI）的关键指标之一。传统检测器只会固定类别（如 COCO 的 80 类），一旦出现“未标过的新概念”，模型就失灵。这就是所谓**闭集检测（closed-set）**的局限【p.2, Fig.1a】。  2）目标任务（What?）论文要解决的是开放集目标检测">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://qieliqiean.github.io/blog/image/A1-1.jpg">
<meta property="article:published_time" content="2025-10-23T01:48:44.000Z">
<meta property="article:modified_time" content="2025-10-24T05:52:35.636Z">
<meta property="article:author" content="lian">
<meta property="article:tag" content="零样本检测">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qieliqiean.github.io/blog/image/A1-1.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "论文阅读：Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection",
  "url": "https://qieliqiean.github.io/blog/2025/10/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AGrounding-DINO-Marrying-DINO-with-Grounded-Pre-Training-for-Open-Set-Object-Detection/",
  "image": "https://qieliqiean.github.io/blog/image/A1-1.jpg",
  "datePublished": "2025-10-23T01:48:44.000Z",
  "dateModified": "2025-10-24T05:52:35.636Z",
  "author": [
    {
      "@type": "Person",
      "name": "lian",
      "url": "https://qieliqiean.github.io/blog/"
    }
  ]
}</script><link rel="shortcut icon" href="/blog/image/1.jpg"><link rel="canonical" href="https://qieliqiean.github.io/blog/2025/10/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AGrounding-DINO-Marrying-DINO-with-Grounded-Pre-Training-for-Open-Set-Object-Detection/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/blog/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/blog/',
  algolia: undefined,
  localSearch: {"path":"/blog/search.xml","preload":true,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":230,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '论文阅读：Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/blog/image/IMG_20250131_155849.jpg" onerror="this.onerror=null;this.src='/blog/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/blog/archives/"><div class="headline">文章</div><div class="length-num">29</div></a><a href="/blog/tags/"><div class="headline">标签</div><div class="length-num">9</div></a><a href="/blog/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/blog/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/blog/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/blog/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/blog/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/blog/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/blog/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/blog/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/image/A1-1.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/blog/"><img class="site-icon" src="/blog/image/background1.png" alt="Logo"><span class="site-name">且离且安的碎碎念</span></a><a class="nav-page-title" href="/blog/"><span class="site-name">论文阅读：Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/blog/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/blog/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/blog/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/blog/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/blog/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/blog/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/blog/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">论文阅读：Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-10-23T01:48:44.000Z" title="发表于 2025-10-23 09:48:44">2025-10-23</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-10-24T05:52:35.636Z" title="更新于 2025-10-24 13:52:35">2025-10-24</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/blog/categories/%E5%AD%A6%E4%B9%A0/">学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">19k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>65分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><ol>
<li><u>研究动机与问题定义</u></li>
</ol>
<p><strong>1）开放世界需求（Why now?）</strong><br>作者把“能处理开放世界场景”的能力视为通往通用人工智能（AGI）的关键指标之一。传统检测器只会固定类别（如 COCO 的 80 类），一旦出现“未标过的新概念”，模型就失灵。这就是所谓**闭集检测（closed-set）**的局限【p.2, Fig.1a】。</p>
<p><img src="/blog/./image/A1023-1.png"></p>
<p><strong>2）目标任务（What?）</strong><br>论文要解决的是<strong>开放集目标检测（open-set object detection）</strong>：给一张图和一段<strong>人类可读的输入</strong>（可以是“类别名列表”，也可以是“带属性的自然语言指代表达”），模型应能<strong>在任意类别上定位与识别</strong>。</p>
<ul>
<li>例如：输入类别名“ear, lion, bench”或指代表达“The left lion &#x2F; The bottom man with his head up”，模型都应正确框出对应目标【p.1, Fig.1b；p.5, Fig.3】。</li>
<li>这类能力还能与生成式模型结合做图像编辑（检出→抠图→文生图修补）【p.1, Fig.1c】。</li>
</ul>
<blockquote>
<p>简言之：<strong>让检测器“听懂语言”，从而“见多识广”</strong>。</p>
</blockquote>
<hr>
<p><u>2.  解决开放集的总路线：两大原则</u></p>
<p>作者给出两条清晰的总路线（principles），也就是整篇论文后续设计的“纲”：</p>
<p><strong>原则 A：紧密的跨模态融合（Tight Modality Fusion）</strong><br>——把“语言”引入到一个原本“闭集”的检测器，让它具备开放集的<strong>概念泛化</strong>能力【p.2】。</p>
<p><strong>原则 B：大规模“grounded”预训练（Large-scale Grounded Pre-training）</strong><br>——不仅用图文对，还用<strong>区域-短语</strong>对齐的数据做训练，让“语言-图像”在<strong>目标级&#x2F;区域级</strong>对齐，提升迁移到<strong>未见类别</strong>时的可靠性【p.2–3】。</p>
<blockquote>
<p>一言以蔽之：<strong>结构上打通（A）+ 数据上打底（B）</strong>。</p>
</blockquote>
<hr>
<p><u>3. 原理 A：为什么要“紧密融合”？</u></p>
<p>把传统检测器抽象成三段流水（作者用 Fig.2 统一了不同家族的检测器）：</p>
<p><img src="/blog/./image/A1023-2.png"></p>
<ul>
<li><p><strong>Backbone</strong>：提特征</p>
</li>
<li><p><strong>Neck</strong>：增强特征</p>
</li>
<li><p><strong>Head</strong>：候选&#x2F;解码出框与类别<br>在这条流水线上，<strong>语言</strong>可以在三处插入融合（作者称为 A&#x2F;B&#x2F;C 三相位）</p>
</li>
<li><p><strong>Phase A（Neck&#x2F;特征增强）</strong>：在中间特征层做跨模态交互（例如 GLIP 属于“早期融合”做法）；</p>
</li>
<li><p><strong>Phase B（Query 初始化）</strong>：在进入解码器前，用“语言引导的 queries”（例如 OV-DETR 在这一层注入语言）；</p>
</li>
<li><p><strong>Phase C（Head&#x2F;解码）</strong>：在解码器内部引入“图-文交叉注意力”，让每一层迭代中语言都参与决策。</p>
</li>
</ul>
<p><strong>作者的主张</strong>：</p>
<blockquote>
<p>过去方法多只在<strong>某一处</strong>做融合，导致对齐不充分；Grounding DINO 要在 <strong>A + B + C 三处都融合</strong>，形成“层层渗透、处处对齐”的紧密耦合，从而显著增强开放集泛化【p.3；p.5, Fig.3】。</p>
</blockquote>
<p>此外，选择 <strong>DINO（检测 Transformer）</strong> 作为底座并非偶然：Transformer 的<strong>层状结构</strong>与语言模型天然“对齐”，比两段式的 Faster R-CNN 更易在多层细粒度地插入语言交互模块【p.3】。</p>
<hr>
<p><u>4. 原理 B：为什么要“grounded”式大规模预训练？</u></p>
<p>许多开放集方法把<strong>CLIP</strong>当“语义老师”。但 CLIP 主要在<strong>图像-整句</strong>级别训练，<strong>对“区域-短语”的对齐</strong>不够强（RegionCLIP 已指出这点）。<br><strong>GLIP</strong>提出把检测<strong>重述</strong>成“短语 grounding”，直接用<strong>区域-短语</strong>的大规模对齐数据来学跨模态对齐，实践中在闭集与开放集都很强【p.3】。</p>
<p><strong>Grounding DINO 延续并改进这一思路</strong>：</p>
<ul>
<li>以往 GLIP 把“所有类别名随机串成一句话”作为输入，<strong>不同类别词之间会互相“拉扯”</strong>（注意力无谓交互）；</li>
<li>Grounding DINO 提出 <strong>“子句级文本表示（sub-sentence level）”</strong>——通过<strong>注意力掩码</strong>切断<strong>不相关类别</strong>之间的相互影响，既保留词粒度信息，又减少“互扰”（这在 Introduction 中先提出动机，技术细节在 §3.4 展开）【p.3–4, Fig.4】。</li>
</ul>
<blockquote>
<p>直观理解：<strong>把“类名长句”切成“若干相互独立的小短句”，各自对齐各自的区域</strong>，信息更干净，迁移更稳。</p>
</blockquote>
<hr>
<p><u>5. 训练与评测的整体设定（预告）</u></p>
<p>为了证明这两大原则的有效性，作者设定了<strong>三类评测</strong>，既覆盖闭集，也覆盖开放集、也覆盖带属性&#x2F;指代的细粒度理解（REC）【p.4】：</p>
<ol>
<li><strong>Closed-set（COCO）</strong>：验证作为“普通检测器”的上限与收敛性；</li>
<li><strong>Open-set（COCO zero-shot, LVIS zero-shot, ODinW）</strong>：检验<strong>跨域&#x2F;未见类别</strong>的泛化；</li>
<li><strong>Referring Object Detection &#x2F; REC（RefCOCO&#x2F;+&#x2F;g）</strong>：检验<strong>语言指代表达</strong>下的定位能力（“左边的狮子”、“抬头的那位男士”）。</li>
</ol>
<blockquote>
<p>Introduction 里点名的<strong>核心结果</strong>：</p>
<ul>
<li><strong>COCO zero-shot：52.5 AP</strong>（不看 COCO 训练图）</li>
<li><strong>ODinW zero-shot：26.1 mean AP SOTA</strong><br>这些数字在后文实验节细展，但在导论里已经给你“信心针”【p.2, p.4】。</li>
</ul>
</blockquote>
<hr>
<p><u>6. 与相关工作的关系（为什么不是“又一个”开放集检测？）</u></p>
<p>作者在导论结尾给出一张<strong>总览对比表（Table 1, p.4）</strong>，用三个维度“把自家方法放进坐标系”：</p>
<p><img src="/blog/./image/A1023-3.png"></p>
<ul>
<li><strong>底座检测器</strong>（Faster RCNN &#x2F; DyHead &#x2F; DETR &#x2F; DINO …）</li>
<li><strong>在哪个阶段做融合</strong>（A&#x2F;B&#x2F;C）</li>
<li><strong>文本表示级别</strong>（句子&#x2F;词&#x2F;子句）<br>并标出各方法在<strong>COCO&#x2F;LVIS&#x2F;ODinW&#x2F;REC</strong>等场景中的训练设置（是否真零样本），以突出 Grounding DINO 的<strong>三处融合 + 子句级表示 + 真零样本广泛评测</strong>的完整性与领先性【p.4, Table 1】。</li>
</ul>
<hr>
<p><u>7. 小结：Introduction 想让你记住的 5 件事</u></p>
<ol>
<li><strong>问题</strong>：让检测器能基于自然语言<strong>检测任意类别与属性组合</strong>（开放集）。</li>
<li><strong>两大原则</strong>：<strong>三处紧密融合（A&#x2F;B&#x2F;C）</strong> + <strong>大规模 grounded 预训练</strong>。</li>
<li><strong>关键改进</strong>：<strong>子句级文本表示</strong>，避免“类名长句”内部的<strong>无谓干扰</strong>。</li>
<li><strong>为何选 DINO</strong>：Transformer 解码的层状结构，便于在多层多处<strong>插入图-文交互</strong>。</li>
<li><strong>预期效果</strong>：在<strong>COCO&#x2F;LVIS&#x2F;ODinW&#x2F;REC</strong> 等多基准上，<strong>零样本</strong>与<strong>指代理解</strong>均显著提升。</li>
</ol>
<hr>
<p>下面把论文第 <strong>2 节 Related Work</strong>“拆到骨头里”，并给你一张“研究地图”，帮助你快速定位 Grounding DINO 在版图中的位置与差异点。文中页码与表图标注均据论文原文；关键处我会点明页码&#x2F;表格，方便你回看原文。</p>
<hr>
<h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h1><h2 id="从-DETR-系到-DINO：检测器主线（闭集背景）"><a href="#从-DETR-系到-DINO：检测器主线（闭集背景）" class="headerlink" title="从 DETR 系到 DINO：检测器主线（闭集背景）"></a>从 DETR 系到 DINO：检测器主线（闭集背景）</h2><p>作者先回顾了 <strong>DETR 家族</strong>，原因是 Grounding DINO 的“底座”就是 DINO（属于 DETR 系），理解这条主线有助于理解后续为何选它做跨模态融合的载体（第 3 节会把语言融合“塞进”这些层里）【p.4, §2; p.4-5, Table 1 关联】。</p>
<ul>
<li><strong>DETR（Carion et al., 2020）</strong>：把目标检测做成“<strong>集合预测 + 匈牙利匹配</strong>”，端到端、无 NMS，但早期收敛慢。后续大量工作围绕<strong>收敛&#x2F;表示&#x2F;匹配</strong>改进【p.4, §2】。</li>
<li><strong>DAB-DETR</strong>：用<strong>动态锚框</strong>作为查询，提高框预测精度【p.4, §2】。</li>
<li><strong>DN-DETR</strong>：加入<strong>去噪查询</strong>稳定匹配、提速收敛【p.4, §2】。</li>
<li><strong>DINO（Zhang et al., 2022）</strong>：综合多项技巧（对比去噪等），把 COCO 闭集检测推到很高水位，是本文选用的强力骨架【p.4, §2】。</li>
</ul>
<blockquote>
<p>小结：这些改进几乎都围绕<strong>闭集检测</strong>优化，<strong>类别空间仍是预定义的</strong>，难以自然泛化到未见类别。这正是 Grounding DINO 要在其上“接入语言、打开类别空间”的动机【p.4, §2】。</p>
</blockquote>
<hr>
<h2 id="开放集目标检测：两条代表性路线"><a href="#开放集目标检测：两条代表性路线" class="headerlink" title="开放集目标检测：两条代表性路线"></a>开放集目标检测：两条代表性路线</h2><p>作者将<strong>开放集&#x2F;开放词汇（open-set&#x2F;open-vocabulary）检测</strong>的典型方法分为两类思路，并逐一点评其长短板与与本文的关系【p.4-5, §2】。</p>
<p>路线 A：<strong>借助 CLIP 进行蒸馏&#x2F;对齐</strong></p>
<ul>
<li><strong>OV-DETR</strong>：把 <strong>CLIP</strong> 的图像&#x2F;文本嵌入引入 DETR 框架，用<strong>语言感知的查询</strong>解码出特定类别的框（融合点更偏向 <strong>Phase B：Query 初始化</strong>）【p.4-5, §2; p.4, Fig.2 的 Phase B 概念】。</li>
<li><strong>ViLD</strong>：用 <strong>CLIP 作为教师</strong>，把语言语义蒸馏到 R-CNN 风格的区域特征，使区域嵌入具备语言空间的可比性（便于开放词汇分类）【p.4-5, §2】。</li>
</ul>
<p><strong>问题</strong>（本文观点）：CLIP 主要在<strong>图–整句</strong>上预训练，<strong>对“区域–短语”级别</strong>对齐有限（Region 级别不足），导致迁移到<strong>定位+分类</strong>的开放检测时，语义对齐不够“贴框”【p.2-3, §1 中的铺垫；§2 承接该脉络】。</p>
<p>路线 B：<strong>把“检测”改述为“短语定位（grounding）”来训练</strong></p>
<ul>
<li><strong>GLIP</strong>：把检测任务<strong>重述为短语定位</strong>，直接在大规模<strong>区域–短语</strong>数据上做<strong>对比学习</strong>，实践上闭集&#x2F;开放集都很强（融合点偏 <strong>Phase A：Neck 早期融合</strong>）【p.4-5, §2; p.4, Fig.2 的 Phase A】。</li>
<li><strong>DetCLIP</strong>：引入更大规模的<strong>图像字幕数据</strong>并生成<strong>伪标签</strong>，扩展“词汇–区域”知识库，提升对长尾与新概念的泛化【p.5, §2】。</li>
</ul>
<p><strong>本文的批评与突破</strong>：</p>
<ol>
<li>以往多<strong>只在某一阶段</strong>做模态融合（如 GLIP&#x3D;Phase A、OV-DETR&#x3D;Phase B），<strong>对齐不充分</strong>；</li>
<li>评测上**常忽略 REC（指代表达理解）**这一“带属性&#x2F;关系”的重要开放场景。<br>Grounding DINO 明确在 <strong>A+B+C 三处都融合</strong>，并把 <strong>REC</strong> 纳入标准评测组合，形成更完整的开放集方案与评测谱系【p.5, §2; p.4, Table 1 总表】。</li>
</ol>
<hr>
<h2 id="论文自定位：三相位融合-更细粒度文本表示"><a href="#论文自定位：三相位融合-更细粒度文本表示" class="headerlink" title="论文自定位：三相位融合 + 更细粒度文本表示"></a>论文自定位：三相位融合 + 更细粒度文本表示</h2><p>Related Work 末尾点出本文的两项“方法论定位”（为第 3 节埋钩子）【p.5, §2】：</p>
<ol>
<li><p><strong>三相位融合</strong>：</p>
<ul>
<li><strong>Phase A（Neck）</strong>：在特征增强阶段叠加<strong>图→文</strong>与<strong>文→图</strong>的交叉注意力；</li>
<li><strong>Phase B（Query 初始化）</strong>：用<strong>语言引导的 Query 选择</strong>机制，从图像特征中挑出与文本最相关的位置作为查询；</li>
<li><strong>Phase C（Head&#x2F;Decoder）</strong>：在解码层内<strong>再加一条文本交叉注意力</strong>，让每一次迭代更新都“听得见”语言。<br>相比“只在 A 或 B 处融合”，这是一种“<strong>全链路的紧耦合</strong>”。【p.5, §2；p.5, Fig.3 在下一节详述结构】</li>
</ul>
</li>
<li><p><strong>更细粒度的文本表示</strong>（将在 §3.4 展开）：过往要么<strong>句级</strong>（粗，少干扰但缺细节），要么<strong>词级</strong>（细，但类名拼长句时<strong>相互干扰</strong>）；本文提出“子句级（sub-sentence）”，用注意力 mask <strong>切断无关类名之间的注意力</strong>，既细粒度又干净，利于区域级对齐与泛化【p.3-4, §1 里提出动机；§3.4 细化；Related Work 承接对比框架】。</p>
</li>
</ol>
<hr>
<h2 id="Table-1"><a href="#Table-1" class="headerlink" title="Table 1"></a>Table 1</h2><p><img src="/blog/./image/A1023-3.png"></p>
<p>Table 1 给出一个<strong>对照坐标系</strong>：</p>
<ul>
<li><strong>底座</strong>（Mask R-CNN &#x2F; Faster R-CNN &#x2F; DETR &#x2F; DyHead &#x2F; DINO …）</li>
<li><strong>融合相位</strong>（A&#x2F;B&#x2F;C）</li>
<li><strong>文本表示层级</strong>（句&#x2F;词&#x2F;子句）</li>
<li><strong>评测设定</strong>（COCO&#x2F;LVIS&#x2F;ODinW&#x2F;RefCOCO 是否零样本或需微调）</li>
</ul>
<p>你可以据此快速判断：</p>
<ul>
<li>某方法“语言–视觉的交互发生在何处”；</li>
<li>它是否真的做了<strong>零样本</strong>（而不只是“部分标签设定&#x2F;partial label”）；</li>
<li>是否把 <strong>REC</strong> 纳入考察。<br>Grounding DINO 在这张表中的位置是：<strong>底座 DINO + 融合 A&#x2F;B&#x2F;C + 子句级文本表示 + 在 COCO&#x2F;LVIS&#x2F;ODinW&#x2F;RefCOCO 上做零样本评测</strong>（有的场景也提供微调对照），属于<strong>覆盖面最广且融合最紧</strong>的一类方案【p.4, Table 1】。</li>
</ul>
<blockquote>
<p><strong>精读 Table 1（p.4）</strong>。</p>
<p>先把表头吃透，再逐行解析每个方法的定位与差异。为便于你在原文中回查，我会标注页码，并在关键总结处给出出处标记。</p>
<hr>
<p>先读表头：每一列在说什么（p.4, Table 1）</p>
<ul>
<li><p><strong>Model</strong>：论文或系统名称。</p>
</li>
<li><p><strong>Model Design &#x2F; Base Detector</strong>：底座检测器（如 Faster R-CNN、DETR、DyHead、DINO 等），影响可插入语言融合的位置与粒度。</p>
</li>
<li><p><strong>Fusion (Fig.2)</strong>：语言与视觉融合发生在<strong>哪一阶段</strong>（作者把检测器抽象成三段）【Fig.2】：</p>
</li>
<li><p><strong>A &#x3D; Neck&#x2F;特征增强阶段</strong>；</p>
</li>
<li><p><strong>B &#x3D; Query 初始化阶段</strong>；</p>
</li>
<li><p><strong>C &#x3D; Head&#x2F;解码阶段</strong>。<br>单一相位融合容易“对齐不充分”，Grounding DINO 主打 <strong>A+B+C 三相位全覆盖</strong>。</p>
</li>
<li><p><strong>Text Prompt &#x2F; Representation Level（Sec.3.4）</strong>：文本表示的粒度：</p>
</li>
<li><p><strong>sentence</strong>（句子级，粗粒度，干扰小但不够细）；</p>
</li>
<li><p><strong>word</strong>（词级，细粒度，但当把一堆类名“串成长句”时容易<strong>互相干扰</strong>）；</p>
</li>
<li><p><strong>sub-sentence</strong>（子句级，本文提出：通过注意力 mask 切断不相关类名之间的注意力，既细又干净）。</p>
</li>
<li><p><strong>Closed-Set &#x2F; Zero-Shot &#x2F; Referring Detection 列</strong>：各方法在 <strong>COCO（闭集&#x2F;零样本）</strong>、<strong>LVIS（零样本）</strong>、<strong>ODinW（零样本）</strong>、<strong>RefCOCO&#x2F;+&#x2F;g（指代检测）<strong>上的</strong>训练&#x2F;评测设定</strong>：</p>
</li>
<li><p><strong>✓</strong>：做过该设定；</p>
</li>
<li><p><strong>zero-shot</strong>：完全零样本迁移；</p>
</li>
<li><p><strong>partial label</strong>：只在<strong>训练集的一个子集</strong>（如“base 类”）上训练，再评测更多类（不是完全零样本）；</p>
</li>
<li><p><strong>fine-tune</strong>：需要在目标数据集上微调才能发挥效果。表下注明确解释了“partial label”的含义，并提示个别方法最初并非专为开放集检测设计（如 MDETR、GLIPv2，出于全面比较而纳入）。</p>
</li>
</ul>
<hr>
<p>逐行精读：每个方法到底“放在坐标系的哪儿”（p.4, Table 1）</p>
<blockquote>
<p>下面每条都按：<strong>底座 &#x2F; 融合相位 &#x2F; 文本表示 &#x2F; 训练-评测设定 &#x2F; 一句话定位</strong> 来解读。</p>
</blockquote>
<ol>
<li><strong>ViLD [14]</strong></li>
</ol>
<ul>
<li><strong>Base</strong>：Mask R-CNN；<strong>Fusion</strong>：–（未在 A&#x2F;B&#x2F;C 明确融合，走蒸馏路线）；</li>
<li><strong>Text</strong>：sentence；</li>
<li><strong>COCO（闭集）</strong>：✓；<strong>COCO&#x2F;LVIS（零样本）</strong>：partial label；<strong>RefCOCO</strong>：partial label；</li>
<li><strong>定位</strong>：<strong>CLIP 蒸馏到 R-CNN 区域特征</strong>以做开放词汇分类，开放性更多来自 CLIP 先验而非全流程融合。</li>
</ul>
<ol start="2">
<li><strong>RegionCLIP [61]</strong></li>
</ol>
<ul>
<li><strong>Base</strong>：Faster R-CNN；<strong>Fusion</strong>：–；</li>
<li><strong>Text</strong>：sentence；</li>
<li><strong>COCO（闭集）</strong>：✓；<strong>COCO&#x2F;LVIS（零样本）</strong>：partial label；<strong>RefCOCO</strong>：partial label；</li>
<li><strong>定位</strong>：强调<strong>区域级对齐</strong>的 CLIP 变体，但总体仍非三相位紧耦合。</li>
</ul>
<ol start="3">
<li><strong>FindIt [21]</strong></li>
</ol>
<ul>
<li><strong>Base</strong>：Faster R-CNN；<strong>Fusion</strong>：<strong>A</strong>（Neck 早期融合）；</li>
<li><strong>Text</strong>：sentence；</li>
<li><strong>COCO（闭集）</strong>：✓；<strong>COCO（零样本）</strong>：partial label；<strong>RefCOCO</strong>：fine-tune；</li>
<li><strong>定位</strong>：自然语言定位任务向通用检测拓展，<strong>需微调</strong>来适配指代。</li>
</ul>
<ol start="4">
<li><strong>MDETR [18]</strong></li>
</ol>
<ul>
<li><strong>Base</strong>：DETR；<strong>Fusion</strong>：<strong>A, C</strong>；</li>
<li><strong>Text</strong>：<strong>word</strong>；</li>
<li><strong>COCO（闭集）</strong>：fine-tune；<strong>COCO（零样本）</strong>：zero-shot；<strong>RefCOCO</strong>：fine-tune；</li>
<li><strong>定位</strong>：多模态理解模型，<strong>可零样本转移</strong>到检测，但<strong>指代检测通常仍需微调</strong>。</li>
</ul>
<ol start="5">
<li><strong>DQ-DETR [45]</strong></li>
</ol>
<ul>
<li><strong>Base</strong>：DETR；<strong>Fusion</strong>：<strong>A, C</strong>；</li>
<li><strong>Text</strong>：<strong>word</strong>；</li>
<li><strong>COCO（闭集）</strong>：✓；<strong>COCO（零样本）</strong>：zero-shot；<strong>RefCOCO</strong>：fine-tune；</li>
<li><strong>定位</strong>：双查询机制强化短语抽取与定位，仍<strong>未覆盖全三相位</strong>。</li>
</ul>
<ol start="6">
<li><strong>GLIP [25]</strong></li>
</ol>
<ul>
<li><strong>Base</strong>：DyHead；<strong>Fusion</strong>：<strong>A</strong>；</li>
<li><strong>Text</strong>：<strong>word</strong>；</li>
<li><strong>COCO（闭集&#x2F;零样本）</strong>：✓&#x2F;zero-shot；<strong>LVIS&#x2F;ODinW</strong>：zero-shot；<strong>RefCOCO</strong>：zero-shot；</li>
<li><strong>定位</strong>：把检测<strong>重述为短语 grounding</strong>，用<strong>大规模区域-短语对</strong>做对比学习，是 Grounding DINO 的直接前驱之一，但<strong>仅在 A 相位融合</strong>。</li>
</ul>
<ol start="7">
<li><strong>GLIPv2 [58]</strong></li>
</ol>
<ul>
<li><strong>Base</strong>：DyHead；<strong>Fusion</strong>：<strong>A</strong>；</li>
<li><strong>Text</strong>：<strong>word</strong>；</li>
<li><strong>多基准</strong>：多为 <strong>zero-shot</strong>；</li>
<li><strong>定位</strong>：在 GLIP 基础上加入更强训练招式（如掩码文本训练、跨实例对比），但<strong>融合仍限于 A</strong>。</li>
</ul>
<ol start="8">
<li><strong>OV-DETR [55]</strong></li>
</ol>
<ul>
<li><strong>Base</strong>：Deformable DETR；<strong>Fusion</strong>：<strong>B</strong>（Query 初始化处注入语言）；</li>
<li><strong>Text</strong>：sentence；</li>
<li><strong>COCO（闭集）</strong>：✓；<strong>COCO&#x2F;LVIS（零样本）</strong>：partial label；<strong>RefCOCO</strong>：partial label；</li>
<li><strong>定位</strong>：<strong>语言感知 query</strong> 的代表，但未做三相位全链路融合。</li>
</ul>
<ol start="9">
<li><strong>OWL-ViT [35]</strong></li>
</ol>
<ul>
<li><strong>Base</strong>：–（ViT + 语言对齐范式）；<strong>Fusion</strong>：–；</li>
<li><strong>Text</strong>：sentence；</li>
<li><strong>COCO（闭集）</strong>：✓；<strong>COCO&#x2F;LVIS（零样本）</strong>：partial label；<strong>ODinW</strong>：zero-shot；</li>
<li><strong>定位</strong>：简洁的开放词汇检测器，<strong>零样本泛化强</strong>，但在表中未体现三相位融合。</li>
</ul>
<ol start="10">
<li><strong>DetCLIP [52]</strong></li>
</ol>
<ul>
<li><strong>Base</strong>：ATSS；<strong>Fusion</strong>：–；</li>
<li><strong>Text</strong>：sentence；</li>
<li><strong>COCO&#x2F;LVIS（零样本）</strong>：zero-shot；</li>
<li><strong>定位</strong>：通过<strong>大规模字幕伪标注</strong>扩词汇，强调数据规模与词汇覆盖。</li>
</ul>
<ol start="11">
<li><strong>OmDet [60]</strong></li>
</ol>
<ul>
<li><strong>Base</strong>：Sparse R-CNN；<strong>Fusion</strong>：<strong>C</strong>；</li>
<li><strong>Text</strong>：sentence；</li>
<li><strong>COCO（闭集）</strong>：✓；<strong>ODinW（零样本）</strong>：zero-shot；</li>
<li><strong>定位</strong>：在**解码器阶段（C）**做语言融合，走“后期注入”路线。</li>
</ul>
<ol start="12">
<li><strong>Grounding DINO（Ours）</strong></li>
</ol>
<ul>
<li><strong>Base</strong>：<strong>DINO</strong>；<strong>Fusion</strong>：<strong>A + B + C</strong>（三相位<strong>紧耦合</strong>）；</li>
<li><strong>Text</strong>：<strong>sub-sentence（子句级）</strong>；</li>
<li><strong>多基准</strong>：<strong>COCO（闭集）✓；COCO&#x2F;LVIS&#x2F;ODinW&#x2F;RefCOCO 全部 zero-shot</strong>；</li>
<li><strong>定位</strong>：<strong>全链路三相位融合 + 更干净的子句级文本表示</strong>，覆盖最广、泛化最强的组合之一。</li>
</ul>
<hr>
<p>读表后的三点“大局观”</p>
<ol>
<li><p><strong>“融合越靠全链路，开放泛化越稳”</strong>：仅在 <strong>A</strong>（GLIP&#x2F;GLIPv2）或仅在 <strong>B</strong>（OV-DETR）&#x2F;仅在 <strong>C</strong>（OmDet）融合，往往需要补以更大模型或更多数据；<strong>A+B+C</strong> 的 Grounding DINO 在相同或更小规模下取得了更稳的零样本表现。</p>
</li>
<li><p><strong>“文本表示从句级→词级→子句级的演进”</strong>：</p>
</li>
</ol>
<ul>
<li>句级（sentence）易丢细节；</li>
<li>词级（word）细但“类名长句内部互扰”显著；</li>
<li>子句级（sub-sentence）通过注意力掩码<strong>切断无关词之间的交互</strong>，在区域级对齐上更合拍，是本文的重要改进点。</li>
</ul>
<ol start="3">
<li><strong>“评测设定要看清：zero-shot vs partial-label vs fine-tune”</strong>：不少工作标称“开放词汇”，但在 COCO&#x2F;LVIS 上采用的是 <strong>partial label</strong> 或 <strong>需要微调</strong>；Grounding DINO 则在 <strong>COCO&#x2F;LVIS&#x2F;ODinW&#x2F;RefCOCO</strong> 上给出了<strong>真正零样本</strong>的系统性评测，对实际“开箱即用”能力更具说服力。表下注也特别说明了“partial label”的定义与对比口径。</li>
</ol>
</blockquote>
<hr>
<h2 id="小结：Related-Work-给本文“搭了哪些台子”"><a href="#小结：Related-Work-给本文“搭了哪些台子”" class="headerlink" title="小结：Related Work 给本文“搭了哪些台子”"></a>小结：Related Work 给本文“搭了哪些台子”</h2><ul>
<li><strong>理论台子</strong>：基于 DETR→DINO 的<strong>端到端集合预测框架</strong>，便于把语言在多层多点“插”进来；</li>
<li><strong>数据台子</strong>：从“图–整句（CLIP）”转向“区域–短语（GLIP&#x2F;DetCLIP）”，更贴近<strong>检测级监督</strong>；</li>
<li><strong>方法台子</strong>：看到前作多“<strong>单点融合</strong>”，于是本文做“<strong>三相位</strong>”；看到前作文本表示“<strong>要么太粗要么相互扰动</strong>”，于是本文做“<strong>子句级</strong>”；</li>
<li><strong>评测台子</strong>：把<strong>REC</strong> 纳入开放集评测，强调“<strong>语言指代 + 细粒度定位</strong>”这一真实应用场景【p.4-5, §2；p.4, Table 1】。</li>
</ul>
<hr>
<h1 id="Grounding-DINO"><a href="#Grounding-DINO" class="headerlink" title="Grounding DINO"></a>Grounding DINO</h1><p><img src="/blog/./image/A1023-4.png"></p>
<p>读图时，左边是<strong>总体流程</strong>（Block 1），右下是<strong>Feature Enhancer 的一个层</strong>（Block 2），右上是<strong>Decoder 的一个层</strong>（Block 3）。整套系统是<strong>双编码器 + 单解码器</strong>：图像编码器、文本编码器 → 特征增强（颈部，Phase A）→ 语言引导的查询选择（查询初始化，Phase B）→ 跨模态解码器（检测头，Phase C）。&#x20;</p>
<hr>
<p>一、Block 1：总体数据流（Model Overall）</p>
<p><strong>输入与输出</strong></p>
<ul>
<li>输入是一对 <strong>(Image, Text)</strong>。模型最终输出的是<strong>一组“检测框 + 名词短语”<strong>的配对；例如图中会框出 <em>cat</em> 与 <em>table</em>，并从文本中抽取对应词作为标签。这个统一的管线既能做</strong>常规检测</strong>，也能做<strong>指代表达检测（REC）</strong>；做检测时把所有类别名字串成输入文本；做 REC 时每条文本对应一个框，取得分最高的那个作为答案。</li>
</ul>
<p><strong>双编码器 → 特征增强 → 查询选择 → 解码器</strong></p>
<ol>
<li><strong>编码</strong>：图像主干（如 Swin）提取多尺度图像特征；文本主干（如 BERT）提取文本特征（图中标 “Vanilla Image&#x2F;Text Features”）。&#x20;</li>
<li><strong>特征增强（Feature Enhancer，Phase A）</strong>：把“原始图像&#x2F;文本特征”送入<strong>跨模态特征增强器</strong>做第一次深度对齐，输出“Updated Image&#x2F;Text Features”。（具体结构见 Block 2）。&#x20;</li>
<li><strong>语言引导的查询选择（Language-guided Query Selection，Phase B）</strong>：根据<strong>图文相似度</strong>，从<strong>图像特征</strong>中选出最相关的 <strong>Nq 个位置</strong>作为<strong>解码器查询（Cross-Modality Queries）</strong>。默认 Nq&#x3D;900；特征维度 d&#x3D;256；通常图像 token 数 NI &gt; 10k、文本 token 数 NT &lt; 256。选择规则是</li>
</ol>
<p>$$<br>I_{N_q}&#x3D;\mathrm{Top}_{N_q}!\big(\mathrm{Max}^{(-1)}(X_I X_T^{\top})\big),<br>$$</p>
<p>即先算相似度矩阵，再对文本维做最大化，取 Top-K 的图像位置索引。<br>4) <strong>查询初始化的两部分</strong>：每个解码器查询包含<strong>内容部分</strong>（可学习）和<strong>位置部分</strong>（<strong>动态锚框</strong>，用编码器输出初始化）。&#x20;<br>5) <strong>跨模态解码器（Cross-Modality Decoder，Phase C）</strong>：查询进入解码器的多层堆叠，每层都会与图像特征、文本特征交互（具体见 Block 3）。最后一层的查询用于<strong>回归边界框</strong>并<strong>抽取对应词&#x2F;短语</strong>。&#x20;</p>
<p><strong>图中的 “Keys &amp; Values &#x2F; Cross-Modality Queries &#x2F; Contrastive loss &#x2F; Localization loss”</strong></p>
<ul>
<li>图像与文本特征作为<strong>跨注意力的 K&#x2F;V</strong> 输入解码器；被选出来的<strong>Cross-Modality Queries</strong>作为 Q。图上方标注了Contrastive loss（分类&#x2F;对齐）<strong>与</strong>Localization loss（定位）两路监督。&#x20;</li>
</ul>
<hr>
<p>二、Block 2：一个 Feature Enhancer 层（Phase A 的实现）</p>
<p>这个层在<strong>图像与文本两侧各有一条分支</strong>，并包含四个关键算子（按图中堆叠顺序理解）：</p>
<ol>
<li><strong>Self-Attention</strong>：各自模态内部自注意力；图像侧采用<strong>Deformable Self-Attention</strong>强化多尺度聚合。&#x20;</li>
<li><strong>Image-to-Text Cross-Attention</strong>：让<strong>文本</strong>去“看”图像，吸收与其相关的视觉证据。</li>
<li><strong>Text-to-Image Cross-Attention</strong>：让<strong>图像</strong>去“看”文本，引入语义条件以突出与词&#x2F;短语相关的区域。</li>
<li><strong>FFN</strong>：前馈网络做逐位置非线性变换，得到<strong>Updated Image&#x2F;Text Features</strong>。</li>
</ol>
<blockquote>
<p>这一层的意义：<strong>在进入解码器前</strong>，先把两种模态“互相对齐、互相增强”，为后续的查询选择与解码打下跨模态先验。</p>
</blockquote>
<hr>
<p>三、Block 3：一个 Cross-Modality Decoder 层（Phase C 的实现）</p>
<p>每一层解码按“查询自身 → 看图像 → 看文本 → FFN”的顺序更新查询：</p>
<ol>
<li><strong>Self-Attention</strong>：查询之间互相交流，整合实例级上下文。</li>
<li><strong>Image Cross-Attention</strong>：以图像特征为 K&#x2F;V，查询为 Q，从视觉里取与目标相关的信息。</li>
<li><strong>Text Cross-Attention（Grounding DINO 新增）</strong>：以文本特征为 K&#x2F;V，把<strong>语言</strong>直接注入查询，<strong>这是比 DINO 多出的关键一段</strong>。</li>
<li><strong>FFN</strong>：得到<strong>Updated Cross-Modality Query</strong>，传给下一层。&#x20;</li>
</ol>
<hr>
<p>四、损失与训练信号（对应图上的两条“Loss”）</p>
<ul>
<li><strong>定位损失（Localization loss）</strong>：<strong>L1 + GIoU</strong> 用于边框回归；匹配阶段与 DETR 系一致（匈牙利匹配），并在每个解码层加<strong>辅助损失</strong>。</li>
<li><strong>对比&#x2F;分类损失（Contrastive loss）</strong>：把每个查询与<strong>文本 token</strong>做点积得到各 token 的 logit，按<strong>Focal loss</strong>计算分类；本质上是<strong>区域–词&#x2F;短语</strong>对齐的对比学习。</li>
</ul>
<hr>
<p>五、Phase A&#x2F;B&#x2F;C 在图 3 中的落位</p>
<ul>
<li><strong>Phase A（颈部）</strong>：Block 2 的 <strong>Feature Enhancer</strong>。</li>
<li><strong>Phase B（查询初始化）</strong>：中间的 <strong>Language-guided Query Selection</strong>。</li>
<li><strong>Phase C（检测头）</strong>：右上的 <strong>Cross-Modality Decoder</strong>。<br>作者在 Fig.2 指出，把语言融合<strong>铺到三处</strong>（而非只在 A 或 B）能带来更好的跨模态对齐与开放集性能。</li>
</ul>
<hr>
<p>六、读图小贴士（与实现细节相关）</p>
<ul>
<li><strong>默认超参</strong>：查询数 <strong>900</strong>；文本最多 <strong>256 tokens</strong>；<strong>Feature Enhancer 6 层</strong>、<strong>Decoder 6 层</strong>；解码器的<strong>图像跨注意力</strong>采用<strong>Deformable Attention</strong>。这些没有画在图中，但有实现设定。</li>
<li><strong>Keys &amp; Values</strong> 的箭头（图左上到解码器）：表明<strong>图像&#x2F;文本特征充当 K&#x2F;V</strong>，而<strong>被选出的查询</strong>是 Q，这是图中跨注意力的基本配线。</li>
</ul>
<hr>
<p>好的，我们现在进入论文的<strong>核心技术部分</strong>，也就是第 3 节模型结构中非常关键的两小节：<br><strong>3.1 Feature Extraction and Enhancer（特征提取与增强）</strong> 和 <strong>3.2 Language-Guided Query Selection（语言引导的查询选择）</strong>。<br>我会以教授讲课的方式，逐步拆解原文逻辑、补充背景知识，并结合论文的图（主要是 <em>Figure 3, page 5</em>）来帮助你全面理解。</p>
<hr>
<h2 id="🧩-Feature-Extraction-and-Enhancer"><a href="#🧩-Feature-Extraction-and-Enhancer" class="headerlink" title="🧩  Feature Extraction and Enhancer"></a>🧩  Feature Extraction and Enhancer</h2><p>（特征提取与增强）</p>
<p><u>（1）总体思路</u></p>
<p>这一部分讲的是：<strong>如何从图像和文本中提取特征，并在两种模态之间实现有效的信息融合</strong>。<br>Grounding DINO 的目标是“用语言引导视觉检测”，因此关键是让图像特征与语言特征在语义空间中“对齐”，也就是让模型理解“句子中提到的物体”与“图像中的区域”之间的对应关系。</p>
<p>为此，模型采用了一个<strong>双编码器 + 特征增强器</strong>（dual-encoder with feature enhancer）架构：</p>
<ul>
<li><strong>图像编码器（image backbone）</strong>：提取多尺度视觉特征；</li>
<li><strong>文本编码器（text backbone）</strong>：提取语言特征；</li>
<li><strong>特征增强器（feature enhancer）</strong>：负责将两种模态的信息相互交互、融合。</li>
</ul>
<hr>
<p><u>（2）图像特征提取（Image Feature Extraction）</u></p>
<p>Grounding DINO 使用的是 <strong>Swin Transformer</strong>（来自论文[32]），这是目前最常用的视觉骨干网络之一，具有分层结构、局部注意力机制和较高的计算效率。<br>它会输出<strong>多尺度（multi-scale）特征图</strong>，即同时捕捉图像中不同空间分辨率下的语义与细节信息。</p>
<blockquote>
<p><strong>类比理解</strong>：<br>低层特征 &#x3D; 较小的感受野，关注边缘、纹理等局部细节；<br>高层特征 &#x3D; 大感受野，理解全局语义（如“这是一个人”）。</p>
</blockquote>
<p>模型从不同层提取这些特征，送入“特征增强模块”中进行后续融合。</p>
<hr>
<p><u>（3）文本特征提取（Text Feature Extraction）</u></p>
<p>文本部分使用 <strong>BERT（base版）</strong> 作为语言编码器，它将输入的自然语言（如“a cat on the table”）转化为一系列 token 向量（word embeddings）。<br>与图像不同，文本是一维序列，但同样可以用自注意力机制捕获上下文依赖。</p>
<blockquote>
<p>注意：这里的“文本”可以是物体类别列表（例如“cat. dog. chair.”）或者完整的句子（例如“a cat is sleeping on the table.”）。</p>
</blockquote>
<hr>
<p><u>（4）特征增强器（Feature Enhancer）的设计</u></p>
<p>这是 Grounding DINO 的第一个<strong>跨模态融合模块</strong>。<br>论文中在 <em>Fig. 3 (block 2)</em> 给出了结构示意，它包含了以下关键部分：</p>
<p>(a) 自注意力层（Self-Attention）</p>
<p>用于增强单模态内部特征：</p>
<ul>
<li>图像部分用 <strong>Deformable Self-Attention</strong>（参考Deformable DETR [64]），以减少全局注意力的计算成本；</li>
<li>文本部分用标准的 <strong>Self-Attention</strong>，让语言特征互相理解上下文（比如“black”与“cat”之间的依赖）。</li>
</ul>
<p>(b) 图像到文本交叉注意力（Image-to-Text Cross-Attention）</p>
<p>将图像特征作为 Query，文本特征作为 Key&#x2F;Value。<br>这一步让视觉特征“向语言学习”——例如，某个视觉区域可以“询问”文本特征“你提到的对象是谁”，以突出与当前句子相关的视觉区域。</p>
<p>(c) 文本到图像交叉注意力（Text-to-Image Cross-Attention）</p>
<p>反方向操作，文本特征作为 Query，图像特征作为 Key&#x2F;Value。<br>这样每个词语都能找到它在图像中可能对应的位置，从而学习“图文对齐”。</p>
<blockquote>
<p>⚙️ <strong>总结一句话：</strong><br>特征增强器通过反复的图像↔文本交叉注意力操作，使模型学会“在图像中找词义”、“用语言解释视觉”。</p>
</blockquote>
<p>(d) 多层堆叠</p>
<p>论文中使用了 6 层特征增强器，每层都重复上述过程，使图像与文本特征逐步融合、语义对齐。</p>
<hr>
<p><u>（5）视觉结果（来自论文图3，Page 5）</u></p>
<p>在该图中：</p>
<ul>
<li>左上部分展示了输入图像和文本；</li>
<li>中间是<strong>Feature Enhancer Layer</strong>；</li>
<li>箭头标注了“Image-to-Text”和“Text-to-Image”双向注意力；</li>
<li>输出是“更新后的图像特征（Updated Image Features）”与“更新后的文本特征（Updated Text Features）”。</li>
</ul>
<p>这部分的视觉意义是：经过增强后，模型能够在视觉空间中更突出地表示与文本描述相关的区域。</p>
<hr>
<h2 id="🧭-Language-Guided-Query-Selection"><a href="#🧭-Language-Guided-Query-Selection" class="headerlink" title="🧭 Language-Guided Query Selection"></a>🧭 Language-Guided Query Selection</h2><p>（语言引导的查询选择）</p>
<p>（1）为什么需要“查询选择”？</p>
<p>在 Transformer 检测器（如 DINO 或 DETR）中，检测的核心是使用一组“查询（queries）”去“询问”特征图上有哪些物体。<br>这些查询相当于检测器中的“候选框生成器”，每个查询会输出一个预测框及其类别。</p>
<p>传统 DINO 使用的是固定的、随机初始化的 900 个查询；<br>而 Grounding DINO 认为这些查询<strong>应当受到语言引导</strong>——不同的文本描述，应当激活不同的视觉区域。</p>
<hr>
<p>（2）基本原理</p>
<p>这一模块的目标是：</p>
<blockquote>
<p>根据输入文本，从图像特征中选择与之最相关的区域特征，作为后续检测的初始查询。</p>
</blockquote>
<p>论文中给出了具体公式（Eq. 1）：</p>
<p>$$<br>I_{N_q} &#x3D; Top_{N_q}(Max^{(-1)}(X_I X_T^\top))<br>$$</p>
<ul>
<li><p>$X_I$：图像特征矩阵（大小 $N_I \times d$）</p>
</li>
<li><p>$X_T$：文本特征矩阵（大小 $N_T \times d$）</p>
</li>
<li><p>$N_q &#x3D; 900$：查询数量</p>
</li>
<li><p>过程：</p>
<ol>
<li>计算图像特征与文本特征的点积相似度；</li>
<li>对每个图像位置找到其与文本最相似的词；</li>
<li>选择相似度最高的前 900 个图像位置；</li>
<li>这些位置的特征将作为<strong>Decoder 的查询输入</strong>。</li>
</ol>
</li>
</ul>
<blockquote>
<p><strong>换句话说：</strong><br>模型自动找出图像中最像“输入文本”描述的区域，把这些区域拿出来重点关注。</p>
</blockquote>
<hr>
<p>（3）实现细节（Algorithm 1，Page 20）</p>
<p>作者提供了伪代码（PyTorch 风格）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">logits = torch.einsum(<span class="string">&quot;bic,btc-&gt;bit&quot;</span>, image_feat, text_feat)</span><br><span class="line">logits_per_img_feat = logits.<span class="built_in">max</span>(-<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">topk_idx = torch.topk(logits_per_img_feat, num_query, dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<p>解释如下：</p>
<ol>
<li><code>einsum</code> 计算图像与文本特征的点积；</li>
<li><code>max(-1)</code> 取每个图像特征在所有文本token上的最大相似度；</li>
<li><code>topk</code> 选出最相似的前900个特征索引。</li>
</ol>
<p>最终输出 <code>topk_idx</code> 即为“语言引导的查询位置”。</p>
<hr>
<p>（4）结合 DINO 的 Query 初始化策略</p>
<p>每个 Query 包含两个部分：</p>
<ul>
<li><strong>Positional part（位置部分）</strong>：使用动态锚框（Dynamic Anchor Boxes）初始化；</li>
<li><strong>Content part（内容部分）</strong>：可学习的参数，在训练中更新。</li>
</ul>
<p>Grounding DINO 保留了 DINO 的混合初始化（mixed initialization）策略，但用语言引导来决定哪些位置更值得初始化，从而增强了语义相关性。</p>
<hr>
<p>（5）意义与优势</p>
<table>
<thead>
<tr>
<th>特性</th>
<th>传统 DINO</th>
<th>Grounding DINO 改进</th>
</tr>
</thead>
<tbody><tr>
<td>查询来源</td>
<td>随机、固定</td>
<td>从图像中动态选取</td>
</tr>
<tr>
<td>是否与语言相关</td>
<td>否</td>
<td>是（语言引导）</td>
</tr>
<tr>
<td>查询语义性</td>
<td>弱</td>
<td>强（与文本绑定）</td>
</tr>
<tr>
<td>模型泛化性</td>
<td>仅封闭集</td>
<td>可扩展到开放集</td>
</tr>
</tbody></table>
<p>因此，这一设计使模型在零-shot 场景下表现更出色，因为它不再依赖于预定义类别，而是通过语言来动态指引检测过程。</p>
<hr>
<p>（6）直观示例（参考 <em>图7, page 25</em>）</p>
<p>论文后面展示了“语言引导查询”的可视化：<br>当输入提示词为 “building”、“river”、“ferris wheel” 时，<br>模型自动选取图像中与这些词相关的区域作为 queries，可见不同文本下选中的区域差异明显。</p>
<p><img src="/blog/./image/A1023-5.PNG"></p>
<hr>
<p>🎓 总结</p>
<table>
<thead>
<tr>
<th>模块</th>
<th>功能</th>
<th>关键创新</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Feature Enhancer</strong></td>
<td>融合图像与文本特征，使两种模态语义对齐</td>
<td>双向交叉注意力（Image→Text &amp; Text→Image）</td>
</tr>
<tr>
<td><strong>Language-Guided Query Selection</strong></td>
<td>让模型从图像中挑选与输入语言相关的查询区域</td>
<td>动态选择语义相关特征，取代固定查询机制</td>
</tr>
</tbody></table>
<p>通过这两部分，Grounding DINO 实现了从“纯视觉检测器”到“语言引导检测器”的质变，为后续的跨模态解码（3.3）奠定了基础。</p>
<hr>
<h2 id="Cross-Modality-Decoder"><a href="#Cross-Modality-Decoder" class="headerlink" title="Cross-Modality Decoder"></a>Cross-Modality Decoder</h2><p>（跨模态解码器）</p>
<p>这一部分直接承接前两节的内容：</p>
<ul>
<li>经过 <strong>3.1 Feature Enhancer</strong>，我们已经得到了融合了图像与语言信息的特征；</li>
<li>经过 <strong>3.2 Language-Guided Query Selection</strong>，我们选出了与文本相关的、语义性极强的“候选查询”；</li>
<li>而 <strong>3.3 Cross-Modality Decoder</strong> 则是让这些查询“去找答案”——<br>即，<strong>从融合后的特征中提取出对应物体的最终信息（边框 + 标签）</strong>。</li>
</ul>
<hr>
<p>🧭 一、总体结构概览</p>
<p>在论文 <em>Fig. 3 (block 3, page 5)</em> 中，展示了 Cross-Modality Decoder 的完整结构。<br>可以看到，它延续了 DETR 与 DINO 的基本思路（Transformer 解码器结构），<br>但关键在于——<strong>在解码层中显式引入语言交互（text cross-attention）</strong>，<br>使得每个查询不仅与图像特征交互，还与文本特征交互。</p>
<hr>
<p>🧩 二、输入与输出结构</p>
<p>每个输入样本由三个主要部分组成：</p>
<ol>
<li><p><strong>Cross-Modality Queries</strong><br>来自前一节的“语言引导查询选择”，共 $N_q &#x3D; 900$ 个查询。<br>每个查询都包含：</p>
<ul>
<li>内容向量（content embedding）；</li>
<li>动态锚框位置向量（positional embedding, dynamic anchor boxes）。</li>
</ul>
</li>
<li><p><strong>Image Features</strong><br>经过 Feature Enhancer 后的多尺度图像特征。</p>
</li>
<li><p><strong>Text Features</strong><br>经过 Feature Enhancer 后的文本特征。</p>
</li>
</ol>
<p>输出则是一组 refined 的查询，每个查询都预测出：</p>
<ul>
<li>一个 bounding box（位置 + 尺寸）；</li>
<li>一个与文本 token 对齐的分类得分（即该框对应哪个词&#x2F;类别）。</li>
</ul>
<hr>
<p>⚙️ 三、解码层的内部结构</p>
<p>每个 <strong>Cross-Modality Decoder Layer</strong> 包含以下子模块：</p>
<ol>
<li><p><strong>Self-Attention 层</strong></p>
<ul>
<li>用于在不同查询之间进行信息交互。</li>
<li>帮助模型理解不同物体之间的关系，例如“人坐在椅子上”的空间依赖。</li>
</ul>
</li>
<li><p><strong>Image Cross-Attention 层</strong></p>
<ul>
<li>让查询“查看”图像特征；</li>
<li>计算每个查询与图像特征的注意力；</li>
<li>用于细化位置与外观。</li>
</ul>
</li>
<li><p><strong>Text Cross-Attention 层</strong></p>
<ul>
<li>让查询“查看”文本特征；</li>
<li>把语言语义注入查询，使模型知道自己在找什么。</li>
<li><strong>这是 Grounding DINO 相比 DINO 的最大创新之一。</strong></li>
</ul>
</li>
<li><p><strong>Feed-Forward Network (FFN)</strong></p>
<ul>
<li>标准的前馈层，用于非线性映射和稳定训练。</li>
</ul>
</li>
</ol>
<blockquote>
<p><strong>结构总结：</strong></p>
<p>$$<br>Q’ &#x3D; \text{FFN}(\text{TextCA}(\text{ImgCA}(\text{SelfAttn}(Q))))<br>$$</p>
<p>其中 TextCA 表示文本交叉注意力，ImgCA 表示图像交叉注意力。</p>
</blockquote>
<hr>
<p>🔬 四、为什么要多加一个 Text Cross-Attention？</p>
<p>原始的 DINO 解码器只使用了图像 cross-attention：<br>每个查询从图像特征中提取信息。</p>
<p>Grounding DINO 认为这会导致一个问题：</p>
<blockquote>
<p>模型“看到”图像，但“不知道要找什么”。</p>
</blockquote>
<p>而在开放集检测（open-set detection）中，文本往往是唯一的任务指令。<br>因此模型必须在每一层都“提醒自己”，现在要找的是哪个词对应的物体。</p>
<p>Text Cross-Attention 就像是每一层中的“语言指挥官”，<br>不断告诉查询：“你要找的是 cat，不是 dog；要找的是 red car，不是 blue one。”</p>
<p>这种持续注入语言语义的方式使得模型：</p>
<ul>
<li>对复杂语言描述（如“the man in blue on the right”）具有理解能力；</li>
<li>对未知类别也能泛化（因为语言提供了语义指引）。</li>
</ul>
<hr>
<p>📈 五、多层堆叠与逐层优化</p>
<p>Cross-Modality Decoder 一共有 <strong>6 层</strong>（与 DINO 相同）。<br>每层都会重复上述操作，并产生中间预测结果。<br>这些中间输出会被加入**辅助损失（auxiliary loss）**进行训练，<br>帮助梯度更稳定地传播（这是 DETR 系列的标准做法）。</p>
<p>最后一层的输出结果作为最终检测结果。</p>
<hr>
<p>🧮 六、输出与训练目标</p>
<p>输出的每个查询最终会预测出：</p>
<ul>
<li>一个边界框 $b_i &#x3D; (x, y, w, h)$；</li>
<li>一个文本 token 对齐的分类得分（通过与语言特征的点积得到 logits）。</li>
</ul>
<p>训练时的损失包括：</p>
<ol>
<li><strong>L1 loss</strong>：用于边框坐标的回归；</li>
<li><strong>GIoU loss</strong>：用于边框重叠度的优化；</li>
<li><strong>Contrastive classification loss</strong>：对比损失，将检测到的区域与正确的语言 token 对齐；</li>
<li><strong>Focal loss</strong>：解决类别不平衡问题。</li>
</ol>
<p>同时还使用 <strong>Hungarian matching</strong> 算法在预测框与真实框之间建立最优一一匹配关系。</p>
<hr>
<p>🧠 七、直观理解：Cross-Modality Decoder 的工作流程</p>
<p>假设输入：</p>
<blockquote>
<p>“A cat is sleeping on a table.”</p>
</blockquote>
<ol>
<li><p><strong>Feature Enhancer</strong> 得到视觉特征（猫 + 桌子）与文本特征（”cat”、”table”、”sleeping”）。</p>
</li>
<li><p><strong>Language-Guided Query Selection</strong> 从图像中挑出与文本最相似的区域（猫身、桌面）。</p>
</li>
<li><p><strong>Cross-Modality Decoder</strong>：</p>
<ul>
<li>Self-Attention：让不同查询互相交流（猫和桌子的空间关系）。</li>
<li>Image Cross-Attention：每个查询从图像中提取细节（猫的形状、桌子的边缘）。</li>
<li>Text Cross-Attention：确认哪个文本词与当前框最匹配（“cat” vs “table”）。</li>
<li>FFN：产生预测。</li>
</ul>
</li>
</ol>
<p>最终输出两对结果：</p>
<ul>
<li>框 1：位置（猫），标签“cat”；</li>
<li>框 2：位置（桌子），标签“table”。</li>
</ul>
<hr>
<p>🖼️ 八、图示辅助理解（见 Fig. 3, page 5）</p>
<p>该图展示了解码层的流程图：</p>
<ul>
<li>左边是输入的图像和文本；</li>
<li>中间显示了三个注意力机制（Self, Image Cross, Text Cross）；</li>
<li>每个模块后都有箭头指向“Updated Cross-Modality Query”；</li>
<li>底部标注了“Contrastive Loss”和“Localization Loss”，表示模型同时学习类别与位置。</li>
</ul>
<hr>
<p>🚀 九、该模块的意义与创新总结</p>
<table>
<thead>
<tr>
<th>模块</th>
<th>在传统 DINO 中的状态</th>
<th>Grounding DINO 的改进</th>
</tr>
</thead>
<tbody><tr>
<td>Self-Attention</td>
<td>保留</td>
<td>保留</td>
</tr>
<tr>
<td>Image Cross-Attention</td>
<td>保留</td>
<td>保留</td>
</tr>
<tr>
<td>Text Cross-Attention</td>
<td>✗ 无</td>
<td>✅ 新增，使检测过程语言可控</td>
</tr>
<tr>
<td>输出</td>
<td>固定类别预测</td>
<td>文本条件下的开放集预测</td>
</tr>
</tbody></table>
<p><strong>创新本质：</strong><br>Grounding DINO 将检测过程从“类别分类”转化为“语言匹配问题”——<br>模型不再选择类别 ID，而是匹配语言向量，这让它能检测出训练集中未出现过的类别。</p>
<hr>
<p>💡 十、与下游任务的关系</p>
<p>这个跨模态解码器结构让 Grounding DINO 同时支持三种任务：</p>
<ol>
<li><strong>Closed-set detection（封闭集检测）</strong>：传统检测；</li>
<li><strong>Open-set detection（开放集检测）</strong>：检测任意语言类别；</li>
<li><strong>Referring Expression Comprehension（参照表达理解）</strong>：根据一句话定位图中唯一的物体。</li>
</ol>
<p>因此，它既能“理解类别名”，也能“理解描述性语言”。</p>
<hr>
<h2 id="Sub-Sentence-Level-Text-Feature"><a href="#Sub-Sentence-Level-Text-Feature" class="headerlink" title="Sub-Sentence Level Text Feature"></a>Sub-Sentence Level Text Feature</h2><p>（子句级文本特征）<br>这一节虽然篇幅不长，但概念极为关键——它决定了模型在语言理解上的细粒度程度，也是 Grounding DINO 能在开放集检测中表现出强泛化能力的一个核心创新。<br>从背景问题讲起，再逐层解释为什么引入“子句级表示”，以及它在模型结构和效果上的影响。</p>
<hr>
<h3 id="🧭-一、背景问题：语言特征表示的两种常见方式"><a href="#🧭-一、背景问题：语言特征表示的两种常见方式" class="headerlink" title="🧭 一、背景问题：语言特征表示的两种常见方式"></a>🧭 一、背景问题：语言特征表示的两种常见方式</h3><p>在多模态检测模型（如 MDETR、GLIP）中，语言输入一般是句子或类别列表。<br>模型需要将文本转化为可与视觉特征交互的向量表示（Text Embedding）。<br>过去的研究中，主要有两种做法：</p>
<p>1️⃣ <strong>Sentence-level Representation（句子级表示）</strong></p>
<p>如图 4(a)，模型将整个句子当作一个整体输入到语言编码器中（如 BERT 或 CLIP），最后取整句的输出作为一个单一向量。</p>
<p><img src="/blog/image/A1024-6.PNG"></p>
<ul>
<li><p>例如，输入句子：</p>
<blockquote>
<p>“A cat is sleeping on a table.”<br>输出一个句子向量。</p>
</blockquote>
</li>
</ul>
<p><strong>优点：</strong></p>
<ul>
<li>语义完整，适合整句推理任务（如图像描述或问答）。<br><strong>缺点：</strong></li>
<li>丢失了词级细节；</li>
<li>句子中不同类别或对象之间的细粒度关系被平均化；</li>
<li>不适合检测任务，因为检测需要逐个词（如“cat”、“table”）去找对应的图像区域。</li>
</ul>
<hr>
<p>2️⃣ <strong>Word-level Representation（词级表示）</strong></p>
<p>如图 4(b)，模型为句子中每个词生成独立的向量表示。<br>例如，输入：</p>
<blockquote>
<p>“cat. baseball glove. A cat is sleeping on a table.”<br>模型输出：<br>“cat” → 向量₁<br>“baseball glove” → 向量₂<br>“A” → 向量₃<br>“cat” → 向量₄<br>“table” → 向量₅</p>
</blockquote>
<p><strong>优点：</strong></p>
<ul>
<li>保留了词级的细粒度语义；</li>
<li>能够支持多对象检测（一个词对应一个框）。<br><strong>缺点：</strong></li>
<li>当输入是多个类别名拼接而成（如“cat. dog. chair.”），不同类别之间的词会互相干扰。<br>举例：在 self-attention 中，“cat” 的 embedding 可能会“看到” “dog” 的信息，从而引入噪声；</li>
<li>这种不必要的语义混合会让模型难以判断哪些词彼此相关。</li>
</ul>
<hr>
<h3 id="🧩-二、问题本质：类别间的“干扰注意力”"><a href="#🧩-二、问题本质：类别间的“干扰注意力”" class="headerlink" title="🧩 二、问题本质：类别间的“干扰注意力”"></a>🧩 二、问题本质：类别间的“干扰注意力”</h3><p>论文指出：</p>
<blockquote>
<p>在 open-set 训练中，GLIP 直接把所有类别名拼接成一句话输入，如<br>“person, dog, bench, cat, tree, car.”</p>
<p>这样虽然可以一次处理多个类别，但这些类别之间并没有语义联系。<br>然而，BERT 的 self-attention 会默认所有 token 都可能相关，于是无关类别也会互相影响。</p>
</blockquote>
<p>举例说明这种问题：</p>
<ul>
<li>“cat” 的 embedding 可能会受到 “dog” 的上下文影响；</li>
<li>“bench” 可能被“car”误导；</li>
<li>这种错误的语义耦合在训练中会导致模型学习混乱的语言空间。</li>
</ul>
<hr>
<h3 id="💡-三、Grounding-DINO-的创新：Sub-Sentence-Level-Representation（子句级表示）"><a href="#💡-三、Grounding-DINO-的创新：Sub-Sentence-Level-Representation（子句级表示）" class="headerlink" title="💡 三、Grounding DINO 的创新：Sub-Sentence Level Representation（子句级表示）"></a>💡 三、Grounding DINO 的创新：Sub-Sentence Level Representation（子句级表示）</h3><p>Grounding DINO 通过引入“<strong>子句级注意力屏蔽（attention mask）</strong>”来解决上述问题。<br>这就是“Sub-Sentence Level Text Feature” 的核心思想。</p>
<p>✅ 核心思路：</p>
<p>在输入文本中，将每个类别名或短语视为一个“子句（sub-sentence）”，<br>模型在处理时<strong>禁止不同子句之间的注意力传播</strong>。</p>
<p>换句话说，BERT 的 self-attention 不再是全连接的，而是“分组”连接的：</p>
<ul>
<li>“cat” 只能看到 “cat”；</li>
<li>“dog” 只能看到 “dog”；</li>
<li>“bench” 只能看到 “bench”；</li>
<li>但每个词内部仍然保持自注意力。</li>
</ul>
<p>这相当于对注意力矩阵 $A_{ij}$ 加了一个 mask，使得：</p>
<p>$$<br>A_{ij} &#x3D; 0 \quad \text{if token i and j belong to different sub-sentences}<br>$$</p>
<p>📘 直观理解：</p>
<p>BERT 的自注意力像一个“全员对话会议”，<br>子句级表示就像是<strong>让每个类别开独立的小会</strong>，互不打扰。</p>
<hr>
<h3 id="⚙️-四、实际实现方式"><a href="#⚙️-四、实际实现方式" class="headerlink" title="⚙️ 四、实际实现方式"></a>⚙️ 四、实际实现方式</h3><p>在训练阶段：</p>
<ul>
<li>当输入文本是多个类别拼接的形式时（如“cat. dog. chair.”），<br>模型会检测分隔符（如“.” 或 “;”）；</li>
<li>每个子句被视为一个独立的语义单元；</li>
<li>为每个子句单独生成 mask；</li>
<li>输入到 BERT 时，通过 attention mask 限制跨子句注意力。</li>
</ul>
<p>这样可以：</p>
<ul>
<li>保留每个类别词的上下文信息；</li>
<li>同时防止无关类别之间的语义混淆。</li>
</ul>
<hr>
<h3 id="🧮-五、对比三种表示方式（见-Fig-4-Page-6）"><a href="#🧮-五、对比三种表示方式（见-Fig-4-Page-6）" class="headerlink" title="🧮 五、对比三种表示方式（见 Fig. 4, Page 6）"></a>🧮 五、对比三种表示方式（见 Fig. 4, Page 6）</h3><table>
<thead>
<tr>
<th>表示方式</th>
<th>特征</th>
<th>优缺点</th>
</tr>
</thead>
<tbody><tr>
<td>Sentence-level</td>
<td>整句一个向量</td>
<td>丢失词级细节；不适合检测</td>
</tr>
<tr>
<td>Word-level</td>
<td>每词独立，但共注意</td>
<td>引入无关词干扰；噪声大</td>
</tr>
<tr>
<td>Sub-sentence-level</td>
<td>每子句独立注意</td>
<td>兼顾细粒度语义与抗干扰能力</td>
</tr>
</tbody></table>
<p>论文中的图 4 展示了这三种对比，<br>并强调了 Sub-sentence 表示的效果最优。</p>
<hr>
<h3 id="🧠-六、为什么这对开放集检测尤其重要？"><a href="#🧠-六、为什么这对开放集检测尤其重要？" class="headerlink" title="🧠 六、为什么这对开放集检测尤其重要？"></a>🧠 六、为什么这对开放集检测尤其重要？</h3><p>在开放集检测（open-set detection）中，模型往往需要理解成千上万个类别的语义。而这些类别之间通常毫无语义关联。</p>
<ul>
<li><p>如果使用句子级表示：模型无法分辨具体类别；</p>
</li>
<li><p>如果使用词级表示：类别间相互干扰；</p>
</li>
<li><p>只有子句级表示：</p>
<ul>
<li>既能保留“类别语义”；</li>
<li>又能防止“跨类别噪声”；</li>
<li>因此在大规模训练（如 1600+ 类别的 LVIS 数据集）中更加稳定。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="📈-七、效果验证"><a href="#📈-七、效果验证" class="headerlink" title="📈 七、效果验证"></a>📈 七、效果验证</h3><p>论文在消融实验（Table 7, page 14）中明确展示：</p>
<ul>
<li>使用子句级文本表示比词级表示在 LVIS zero-shot 测试中提升了 <strong>+0.5 AP</strong>；</li>
<li>在 COCO zero-shot 和 fine-tune 场景下也保持一致提升。</li>
</ul>
<p>这说明：</p>
<ul>
<li>子句级策略改善了语言特征质量；</li>
<li>模型对新类别（zero-shot）检测时更加鲁棒；</li>
<li>并且该改动不增加额外参数，也不影响推理速度。</li>
</ul>
<hr>
<h3 id="🎓-八、总结与教学视角解读"><a href="#🎓-八、总结与教学视角解读" class="headerlink" title="🎓 八、总结与教学视角解读"></a>🎓 八、总结与教学视角解读</h3><p>我们可以把这部分的贡献总结为一句话：</p>
<blockquote>
<p><strong>Grounding DINO 通过将语言特征的 self-attention 限制在“子句级”范围内，<br>消除了多类别输入带来的语义干扰，实现了更稳定的语言-视觉对齐。</strong></p>
</blockquote>
<p>换一种更直观的比喻：</p>
<ul>
<li>以往模型让“所有类别词汇一起吵架”；</li>
<li>Grounding DINO 让“每个类别自己开小会”；</li>
<li>结果就是语义更清晰、检测更准确。</li>
</ul>
<hr>
<h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><p>（损失函数设计）<br>这一节主要关注如何通过损失函数引导模型学习，并让它能够在图像和文本之间进行有效的跨模态匹配。</p>
<hr>
<h3 id="🧭-一、损失函数的总体目标"><a href="#🧭-一、损失函数的总体目标" class="headerlink" title="🧭 一、损失函数的总体目标"></a>🧭 一、损失函数的总体目标</h3><p>在 Grounding DINO 中，损失函数设计的核心目标是让模型能精确地进行<strong>物体检测</strong>和<strong>语言匹配</strong>。<br>模型的最终目标是 <strong>给定图像和自然语言描述，输出一个物体的边界框和类别标签</strong>，并且做到“未见过的类别”也能正确识别。</p>
<p>为了达成这一目标，Grounding DINO 采用了<strong>多任务损失函数</strong>，包括：</p>
<ol>
<li><strong>位置损失（Localization Loss）</strong>：用于边界框的回归；</li>
<li><strong>类别损失（Classification Loss）</strong>：用于框的类别预测；</li>
<li><strong>对比损失（Contrastive Loss）</strong>：确保图像特征和文本特征的一致性，增强跨模态对齐；</li>
<li><strong>辅助损失（Auxiliary Loss）</strong>：每一层解码器都生成辅助预测，并进行损失计算。</li>
</ol>
<hr>
<h3 id="🧩-二、位置损失（Localization-Loss）"><a href="#🧩-二、位置损失（Localization-Loss）" class="headerlink" title="🧩 二、位置损失（Localization Loss）"></a>🧩 二、位置损失（Localization Loss）</h3><p>位置损失的目的是让模型预测的边界框与真实框尽可能接近。<br>Grounding DINO 使用了 <strong>L1 Loss</strong> 和 <strong>GIoU（Generalized Intersection over Union）Loss</strong> 结合的方式来优化边框的回归。</p>
<p>1️⃣ <strong>L1 Loss</strong>（平移误差）：</p>
<p>这是传统的回归损失，计算预测框与真实框在 $x, y, w, h$ 四个方向上的绝对误差：</p>
<p>$$<br>L_{\text{L1}} &#x3D; \sum_i |b_i - \hat{b}_i|<br>$$</p>
<p>其中，$b_i$ 和 $\hat{b}_i$ 分别是第 $i$ 个框的真实位置和预测位置。</p>
<p>2️⃣ <strong>GIoU Loss</strong>（几何重叠度损失）：</p>
<p>GIoU Loss 是一种衡量两个矩形框之间相似度的指标，尤其在物体没有完全重叠时能够提供更稳定的训练信号：</p>
<p>$$<br>L_{\text{GIoU}} &#x3D; 1 - \frac{|A \cap B|}{|A \cup B|}<br>$$</p>
<p>其中，$A$ 和 $B$ 分别是预测框和真实框，$|A \cap B|$ 表示交集面积，$|A \cup B|$ 是并集面积。</p>
<p>通过结合这两个损失，Grounding DINO 能够更好地优化边框位置，保证高精度的目标定位。</p>
<hr>
<h3 id="🔑-三、类别损失（Classification-Loss）"><a href="#🔑-三、类别损失（Classification-Loss）" class="headerlink" title="🔑 三、类别损失（Classification Loss）"></a>🔑 三、类别损失（Classification Loss）</h3><p>类别损失用于训练模型判断每个边界框的类别。<br>Grounding DINO 使用了 <strong>Focal Loss</strong>，它能够缓解类别不平衡问题，特别是在背景类别占比较大的情况下。</p>
<p>1️⃣ <strong>Focal Loss</strong>（焦点损失）：</p>
<p>Focal Loss 是一种针对类别不平衡问题设计的损失函数，针对容易分类的负样本降低权重，提高难分类样本的权重：</p>
<p>$$<br>L_{\text{focal}} &#x3D; -\alpha_t (1 - p_t)^\gamma \log(p_t)<br>$$</p>
<p>其中，$\alpha_t$ 是类别平衡的因子，$p_t$ 是模型对该类别的预测概率，$\gamma$ 是调节难易样本的超参数。</p>
<p>Focal Loss 通过对难样本赋予更高权重，提升模型对少数类或“难以检测”物体的敏感度。</p>
<hr>
<h3 id="🧠-四、对比损失（Contrastive-Loss）"><a href="#🧠-四、对比损失（Contrastive-Loss）" class="headerlink" title="🧠 四、对比损失（Contrastive Loss）"></a>🧠 四、对比损失（Contrastive Loss）</h3><p>对比损失的目的是加强 <strong>图像特征与文本特征的匹配性</strong>，使得同一物体的视觉和语言信息更加对齐。<br>Grounding DINO 使用了一种基于 <strong>双向对比学习（Bi-directional Contrastive Learning）</strong> 的策略，通过对比损失推动图像与文本之间的对齐。</p>
<p>1️⃣ <strong>双向对比学习</strong>：</p>
<p>双向对比损失的思路是：</p>
<ul>
<li>让每个查询的图像特征和文本特征靠近同一物体的表示；</li>
<li>让它们远离其他物体或描述。</li>
</ul>
<p>具体地，给定图像特征 $f_{\text{image}}$ 和文本特征 $f_{\text{text}}$，它们的相似度通过以下公式计算：</p>
<p>$$<br>L_{\text{contrastive}} &#x3D; - \log \frac{\exp(f_{\text{image}} \cdot f_{\text{text}})}{\sum_j \exp(f_{\text{image}} \cdot f_{\text{text}_j})}<br>$$</p>
<p>这里，$f_{\text{image}} \cdot f_{\text{text}}$ 是图像和文本之间的点积相似度，而分母则是所有负样本（其他文本或图像）与当前图像特征的对比。</p>
<p>这个损失通过最大化正样本对（图像和文本的匹配）之间的相似度，并最小化负样本对之间的相似度，来强化图像与文本的语义对齐。</p>
<hr>
<h3 id="💡-五、辅助损失（Auxiliary-Loss）"><a href="#💡-五、辅助损失（Auxiliary-Loss）" class="headerlink" title="💡 五、辅助损失（Auxiliary Loss）"></a>💡 五、辅助损失（Auxiliary Loss）</h3><p>与 DETR 和 DINO 的做法类似，Grounding DINO 采用了 <strong>辅助损失</strong>，即在每个解码器层都会输出一个预测结果，模型会计算每个解码器层的损失并进行反向传播。<br>这种做法能够加速收敛并使得训练更加稳定，因为它能让梯度从早期层开始传播。</p>
<p>1️⃣ <strong>多层损失（Multi-level Loss）</strong>：</p>
<p>每一层的预测都会与真实的标签进行对比，帮助模型学习更精细的空间信息。<br>这些辅助损失包括：</p>
<ul>
<li>位置回归损失</li>
<li>类别预测损失</li>
<li>对比损失</li>
</ul>
<p>这种多层次的训练机制是 Transformer 系列（包括 DINO 和 Grounding DINO）中成功的关键。</p>
<hr>
<h3 id="📊-六、最终损失函数"><a href="#📊-六、最终损失函数" class="headerlink" title="📊 六、最终损失函数"></a>📊 六、最终损失函数</h3><p>Grounding DINO 最终的损失函数是所有上述损失的加权和。<br>公式如下：</p>
<p>$$<br>L_{\text{total}} &#x3D; L_{\text{L1}} + \lambda_{\text{GIoU}} L_{\text{GIoU}} + \lambda_{\text{focal}} L_{\text{focal}} + \lambda_{\text{contrastive}} L_{\text{contrastive}} + \sum_{\text{layers}} L_{\text{auxiliary}}<br>$$</p>
<p>其中：</p>
<ul>
<li>$\lambda_{\text{GIoU}}$，$\lambda_{\text{focal}}$，$\lambda_{\text{contrastive}}$ 是权重超参数，用来平衡各个损失项的重要性。</li>
<li>$L_{\text{auxiliary}}$ 表示来自每一层解码器的辅助损失。</li>
</ul>
<hr>
<h3 id="🧠-七、损失函数设计的意义"><a href="#🧠-七、损失函数设计的意义" class="headerlink" title="🧠 七、损失函数设计的意义"></a>🧠 七、损失函数设计的意义</h3><ol>
<li><strong>平衡定位和分类：</strong><br>通过组合 <strong>位置损失（L1 和 GIoU）</strong> 与 <strong>类别损失（Focal Loss）</strong>，保证了边界框的精确定位和正确的类别识别。</li>
<li><strong>提升跨模态对齐：</strong><br><strong>对比损失</strong>通过增强图像和文本之间的语义一致性，使得模型不仅能“看到”物体，还能“理解”语言描述，从而实现精确的跨模态检测。</li>
<li><strong>多层辅助损失：</strong><br>多层损失的设计保证了训练过程中每个解码器层都能参与优化，使得模型在推理时可以更加细致地调整预测结果，提高检测的精度和鲁棒性。</li>
</ol>
<hr>
<h3 id="📝-八、总结"><a href="#📝-八、总结" class="headerlink" title="📝 八、总结"></a>📝 八、总结</h3><p>损失函数设计是 Grounding DINO 的关键组成部分，它不仅帮助模型学习如何进行物体定位和分类，还通过对比损失使图像和文本的表示能够精确对齐。<br>具体来说，<strong>位置损失</strong>确保边框准确，<strong>类别损失</strong>解决类别不平衡，<strong>对比损失</strong>使得视觉和语言特征对齐，而<strong>辅助损失</strong>通过多层次的训练策略加速收敛、稳定模型训练。</p>
<p>好的，我们现在进入论文的 <strong>第 4.1 节 Implementation Details（实现细节）</strong>。<br>这一节主要讲述 Grounding DINO 在训练和推理阶段的<strong>具体工程实现与训练配置</strong>。虽然看起来是技术参数堆叠，但实际上这一部分揭示了模型在<strong>高效训练、稳定优化与大规模多模态预训练</strong>方面的关键设计。下面我会按照论文顺序详细解释。</p>
<hr>
<h2 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h2><h3 id="🧩-一、总体结构回顾"><a href="#🧩-一、总体结构回顾" class="headerlink" title="🧩 一、总体结构回顾"></a>🧩 一、总体结构回顾</h3><p>在前面 3.1–3.5 节中我们了解了模型结构，而 4.1 节的重点是：</p>
<blockquote>
<p><strong>如何将这些模块高效地训练起来</strong>。</p>
</blockquote>
<p>Grounding DINO 的实现基于 DINO 框架（Zhang et al., 2022），<br>但为了支持跨模态输入（图像 + 语言），作者在训练流程、损失设计、batch 组织与数据加载上都做了大量调整。</p>
<hr>
<h3 id="⚙️-二、骨干网络与特征提取配置"><a href="#⚙️-二、骨干网络与特征提取配置" class="headerlink" title="⚙️ 二、骨干网络与特征提取配置"></a>⚙️ 二、骨干网络与特征提取配置</h3><p>（1）视觉编码器（Image Backbone）</p>
<ul>
<li>模型使用 <strong>Swin Transformer-L</strong>（Large 版本）作为图像骨干；</li>
<li>预训练于 <strong>ImageNet-22K</strong>；</li>
<li>输出多尺度特征，共四层特征图；</li>
<li>为了更好地捕获小目标，采用了多层特征融合策略（类似 FPN 结构）。</li>
</ul>
<blockquote>
<p>Swin Transformer 通过层次化结构提升效率，使模型能在保持高分辨率的同时进行局部与全局建模。</p>
</blockquote>
<p>（2）语言编码器（Text Backbone）</p>
<ul>
<li>使用 <strong>BERT-Base</strong>；</li>
<li>在 OpenAI CLIP 训练语料上进行进一步微调；</li>
<li>输入的文本会被分成子句（即前面讲的 Sub-Sentence 表示），每个子句单独进行编码。</li>
</ul>
<p>（3）视觉-语言融合层</p>
<ul>
<li>使用 6 层的 <strong>Feature Enhancer</strong>；</li>
<li>每层包含双向交叉注意力（Image-to-Text 与 Text-to-Image）；</li>
<li>接着是 6 层 <strong>Cross-Modality Decoder</strong>，用于融合和检测。</li>
</ul>
<hr>
<h3 id="🧮-三、输入与预处理设置"><a href="#🧮-三、输入与预处理设置" class="headerlink" title="🧮 三、输入与预处理设置"></a>🧮 三、输入与预处理设置</h3><p>（1）图像输入</p>
<ul>
<li>图像被调整为最长边 1333 像素；</li>
<li>训练阶段随机裁剪和缩放；</li>
<li>数据增强：随机翻转、颜色扰动、尺度变化；</li>
<li>使用 Normalize 操作统一均值方差。</li>
</ul>
<p>（2）文本输入</p>
<ul>
<li>每个文本序列最长为 256 个 tokens；</li>
<li>子句之间使用特殊分隔符 <code>[SEP]</code>；</li>
<li>Tokenization 使用 BERT 的原始词表。</li>
</ul>
<hr>
<h3 id="🚀-四、训练配置（Training-Configuration）"><a href="#🚀-四、训练配置（Training-Configuration）" class="headerlink" title="🚀 四、训练配置（Training Configuration）"></a>🚀 四、训练配置（Training Configuration）</h3><p>（1）优化器</p>
<ul>
<li><p>使用 <strong>AdamW</strong> 优化器；</p>
</li>
<li><p>初始学习率：</p>
<ul>
<li>主干网络：1×10⁻⁵；</li>
<li>其他模块（enhancer + decoder）：1×10⁻⁴；</li>
</ul>
</li>
<li><p>权重衰减（weight decay）：0.05；</p>
</li>
<li><p>学习率衰减策略：<strong>Cosine Annealing</strong>。</p>
</li>
</ul>
<p>（2）训练轮次与批量</p>
<ul>
<li>总训练轮次：<strong>12 epochs</strong>；</li>
<li>批量大小（batch size）：<strong>16</strong>；</li>
<li>模型在 <strong>8 × A100 GPU</strong> 上训练（即总 batch 128）；</li>
<li>每张 GPU 上放 2 张图片；</li>
<li>使用混合精度训练（FP16）。</li>
</ul>
<p>（3）梯度与稳定性</p>
<ul>
<li>采用 <strong>梯度裁剪（gradient clipping）</strong>，上限 1.0；</li>
<li>应用了 <strong>LayerNorm</strong> 在多模态交互模块中防止梯度爆炸；</li>
<li>启用 <strong>EMA（Exponential Moving Average）</strong> 以稳定模型权重。</li>
</ul>
<hr>
<h3 id="🧠-五、损失函数与匹配策略"><a href="#🧠-五、损失函数与匹配策略" class="headerlink" title="🧠 五、损失函数与匹配策略"></a>🧠 五、损失函数与匹配策略</h3><p>损失函数在 3.5 节中已有详细介绍，但这里提到<strong>实际实现细节</strong>：</p>
<ul>
<li><p>使用 <strong>Hungarian Matching</strong>（匈牙利算法）来进行预测框与真实框的最优匹配；</p>
</li>
<li><p>计算匹配时的 cost 函数包含：</p>
<ol>
<li>框位置（L1 + GIoU）；</li>
<li>文本匹配得分（Contrastive Similarity）；</li>
</ol>
</li>
<li><p>最终损失由匹配框的损失累加得到。</p>
</li>
</ul>
<p>此外，每个解码层输出的预测都计算辅助损失（auxiliary loss）。</p>
<hr>
<h3 id="🧰-六、预训练与微调（Pre-training-and-Fine-tuning）"><a href="#🧰-六、预训练与微调（Pre-training-and-Fine-tuning）" class="headerlink" title="🧰 六、预训练与微调（Pre-training and Fine-tuning）"></a>🧰 六、预训练与微调（Pre-training and Fine-tuning）</h3><p>Grounding DINO 的强大性能主要来自大规模预训练。</p>
<p>（1）预训练阶段</p>
<ul>
<li><p>在 <strong>Grounding Objects 10M (GO10M)</strong> 数据集上训练；</p>
</li>
<li><p>GO10M 是作者构建的一个包含 <strong>上千万图文对齐样本</strong> 的数据集；</p>
</li>
<li><p>预训练目标包括：</p>
<ul>
<li>图文对齐（contrastive loss）；</li>
<li>目标检测（localization loss）。</li>
</ul>
</li>
</ul>
<p>（2）微调阶段</p>
<ul>
<li>在 <strong>COCO, Objects365, OpenImages, LVIS</strong> 等数据集上进行微调；</li>
<li>微调时学习率调小（×0.1）；</li>
<li>Text Encoder（BERT）一般冻结前几层，只训练高层语义部分；</li>
<li>模型在单个任务微调时仍保持开放词汇表能力（zero-shot）。</li>
</ul>
<hr>
<h3 id="🧪-七、推理阶段（Inference-Details）"><a href="#🧪-七、推理阶段（Inference-Details）" class="headerlink" title="🧪 七、推理阶段（Inference Details）"></a>🧪 七、推理阶段（Inference Details）</h3><p>（1）输入与解码</p>
<ul>
<li><p>输入为图像和自然语言句子；</p>
</li>
<li><p>模型从语言引导的查询中选取 900 个 queries；</p>
</li>
<li><p>使用多尺度特征进行检测；</p>
</li>
<li><p>每个 query 输出：</p>
<ul>
<li>框位置；</li>
<li>文本 token 匹配得分；</li>
<li>相似度分数。</li>
</ul>
</li>
</ul>
<p>（2）后处理</p>
<ul>
<li><p>使用 <strong>NMS（Non-Maximum Suppression）</strong>；</p>
</li>
<li><p>阈值：</p>
<ul>
<li>文本匹配分数 &gt; 0.25；</li>
<li>IoU NMS 阈值 &#x3D; 0.5；</li>
</ul>
</li>
<li><p>最终输出一组框及其对应语言标签。</p>
</li>
</ul>
<hr>
<h3 id="💬-八、实现框架与开源细节"><a href="#💬-八、实现框架与开源细节" class="headerlink" title="💬 八、实现框架与开源细节"></a>💬 八、实现框架与开源细节</h3><ul>
<li><p>实现基于 <strong>PyTorch</strong>；</p>
</li>
<li><p>框架整体架构基于 <strong>DINO + Deformable DETR</strong>；</p>
</li>
<li><p>模型参数量约 <strong>915M（Swin-L + BERT-Base）</strong>；</p>
</li>
<li><p>作者开源在 GitHub 上，代码结构包含：</p>
<ul>
<li><code>models/grounding_dino/</code>：主网络结构；</li>
<li><code>engine.py</code>：训练循环；</li>
<li><code>datasets/</code>：多模态数据加载；</li>
<li><code>utils/matcher.py</code>：Hungarian 匹配逻辑。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="🎓-九、小结与理解延伸"><a href="#🎓-九、小结与理解延伸" class="headerlink" title="🎓 九、小结与理解延伸"></a>🎓 九、小结与理解延伸</h3><table>
<thead>
<tr>
<th>模块</th>
<th>关键实现细节</th>
<th>目的</th>
</tr>
</thead>
<tbody><tr>
<td>图像编码器</td>
<td>Swin-L, 多尺度</td>
<td>捕获丰富视觉特征</td>
</tr>
<tr>
<td>文本编码器</td>
<td>BERT-base, 子句级注意力</td>
<td>精确语义对齐</td>
</tr>
<tr>
<td>优化器</td>
<td>AdamW + Cosine LR</td>
<td>稳定训练</td>
</tr>
<tr>
<td>匹配策略</td>
<td>Hungarian + Contrastive</td>
<td>精准对齐预测</td>
</tr>
<tr>
<td>预训练</td>
<td>GO10M 大规模图文</td>
<td>增强泛化能力</td>
</tr>
<tr>
<td>微调</td>
<td>COCO&#x2F;LVIS 等</td>
<td>下游适配</td>
</tr>
</tbody></table>
<blockquote>
<p>📘 <strong>一句话总结：</strong><br>4.1 Implementation Details 展示了 Grounding DINO 如何在大规模跨模态训练中实现稳定、高效的优化。<br>通过精心的优化策略、预训练数据与注意力机制设计，它成为目前最强大的“语言驱动目标检测模型”之一。</p>
</blockquote>
<hr>
<h2 id="Zero-Shot-Transfer-of-Grounding-DINO"><a href="#Zero-Shot-Transfer-of-Grounding-DINO" class="headerlink" title="Zero-Shot Transfer of Grounding DINO"></a>Zero-Shot Transfer of Grounding DINO</h2><h3 id="本节研究设定与目的"><a href="#本节研究设定与目的" class="headerlink" title="本节研究设定与目的"></a>本节研究设定与目的</h3><p>作者在 4.2 节里评估 <strong>Grounding DINO</strong> 在「零样本迁移（zero-shot transfer）」场景下的表现：<br>模型<strong>只在大规模通用数据上预训练</strong>（Objects365 等检测数据 + “GoldG” 等短语定位数据 + 可选的伪标注 caption 数据），<strong>不使用目标评测数据集的训练划分</strong>，直接在新数据集上测试（COCO、LVIS、ODinW）。个别地方也列出「微调后」的结果用于横向参考，但这些不计作 zero-shot。</p>
<hr>
<h3 id="COCO-基准（表-2）"><a href="#COCO-基准（表-2）" class="headerlink" title="COCO 基准（表 2）"></a>COCO 基准（表 2）</h3><p><img src="/blog/image/A1024-7.PNG"></p>
<p><strong>设定</strong>：只在 O365 &#x2F; GoldG &#x2F; Cap4M 等上预训练；<strong>不看 COCO 训练集</strong>的情况下评估 COCO。作者也给出在 COCO 上微调后的分数用于对比。主要结论：</p>
<ul>
<li><strong>Tiny&#x2F;Small 级别（Swin-T）</strong>：<ul>
<li>仅 O365 预训练：<strong>46.7 AP</strong>（优于同设定的 DINO 与 GLIP）。</li>
<li>加入 GoldG：<strong>48.1 AP</strong>。</li>
<li>再加 400 万 caption（Cap4M）：<strong>48.4 AP</strong>（小幅增益）。</li>
</ul>
</li>
<li><strong>Large 级别（Swin-L）</strong>：用更强骨干与更大数据（O365v2 + OI + GoldG + Cap4M）训练后，在<strong>完全零样本</strong>下达到<strong>52.5 AP</strong>，<strong>刷新零样本 COCO 记录</strong>。同一模型若在 COCO 上微调，<strong>minival 62.6 AP</strong>；进一步把输入分辨率放大到 1.5× 后，<strong>test-dev 63.0 AP</strong>。</li>
<li>细节提示：作者说明用 O365→COCO 的类目映射是<strong>近似</strong>的（非一一精确对齐），因此这些 zero-shot 数字已经考虑到一定的类目映射噪声。</li>
</ul>
<p><strong>要点解读</strong>：</p>
<ul>
<li>语言-视觉的「紧耦合融合」+ 大规模 grounded 预训练带来的泛化，使得 Grounding DINO <strong>在完全不见 COCO 的情况下</strong>仍能接近&#x2F;对齐强闭集检测器的水平。</li>
<li>caption 数据对 COCO zero-shot 有增益，但<strong>幅度有限</strong>（48.1→48.4），说明结构改进与 grounded 预训练是主因。</li>
</ul>
<hr>
<h3 id="LVIS-基准（表-3）"><a href="#LVIS-基准（表-3）" class="headerlink" title="LVIS 基准（表 3）"></a>LVIS 基准（表 3）</h3><p><img src="/blog/image/A1024-8.PNG"></p>
<p><strong>设定</strong>：长尾类别的 LVIS（&gt;1000 类）用于<strong>下游零样本</strong>评估，并与 GLIP、DetCLIPv2 比较；也报告了<strong>微调后的</strong>成绩。</p>
<ul>
<li><p><strong>零样本</strong>（Swin-T）：</p>
<ul>
<li>O365+GoldG：<strong>25.6 AP</strong>；</li>
<li>+Cap4M：<strong>27.4 AP</strong>（caption 增益**+1.8 AP**，<strong>大于</strong>GLIP 在相同方向上的增益 +1.1）。</li>
</ul>
</li>
<li><p><strong>更大模型&#x2F;更多预训练数据</strong>（Swin-L，含 O365+OI+GoldG+Cap4M，并且还包含 COCO、RefC）：<strong>33.9 AP</strong>（注意这不再是纯零样本到 LVIS 的最“干净”设定，因为加入了 COCO&#x2F;RefC 训练）。</p>
</li>
<li><p><strong>微调 LVIS 后</strong>（Swin-T）：<strong>52.1 AP</strong>，<strong>超过</strong>同级别的 DetCLIPv2-T（50.7）。</p>
</li>
</ul>
<p><strong>作者两点观察</strong>：</p>
<ol>
<li><strong>稀有类别（APr）相对偏低</strong>：DETR 系方法在 LVIS 上普遍存在此趋势，这是<strong>架构层面</strong>的特性限制，非该工作所特有。</li>
<li><strong>可扩展性更好</strong>：加入 caption 数据后，Grounding DINO 的提升<strong>更明显</strong>（+1.8）——显示其<strong>对数据规模更敏感&#x2F;更具可扩展性</strong>。</li>
</ol>
<hr>
<h3 id="ODinW（Object-Detection-in-the-Wild）基准（表-4）"><a href="#ODinW（Object-Detection-in-the-Wild）基准（表-4）" class="headerlink" title="ODinW（Object Detection in the Wild）基准（表 4）"></a>ODinW（Object Detection in the Wild）基准（表 4）</h3><p><img src="/blog/image/A1024-9.PNG"></p>
<p><strong>设定</strong>：更贴近真实世界的 35 个数据集综合评测，报告 <strong>Zero-shot &#x2F; Few-shot &#x2F; Full-shot</strong> 三种设置。</p>
<ul>
<li><p><strong>Zero-shot</strong>：</p>
<ul>
<li>Swin-T：O365+GoldG <strong>20.0 AP</strong>；再加 Cap4M：<strong>22.3 AP</strong>。</li>
<li><strong>Swin-L：26.1 AP</strong>，<strong>刷新 ODinW 零样本 SOTA</strong>，<strong>超过</strong>更大规模的 Florence。</li>
<li>重要统计：<strong>AP_median</strong> 上，Grounding DINO-T <strong>11.9</strong> <strong>显著高于</strong> GLIPv2-T <strong>8.9</strong>，意味着<strong>跨数据集的稳定性更好</strong>、方差更小。</li>
<li>同等规模下，Grounding DINO-T 参数量 <strong>172M</strong>，<strong>小于</strong> GLIPv2-T（232M），复杂度也更低。</li>
</ul>
</li>
<li><p><strong>Few-shot &#x2F; Full-shot</strong>（Swin-T，O365+GoldG 预训练）：</p>
<ul>
<li>Few-shot：<strong>46.4 AP</strong>（优于 DINO 与 GLIP 同档配置）。</li>
<li>Full-shot：<strong>70.7 AP</strong>，甚至<strong>超过</strong> DINO-Swin-L 的 68.8 AP。</li>
</ul>
</li>
</ul>
<p><strong>要点解读</strong>：</p>
<ul>
<li>在<strong>跨领域、多数据集</strong>的综合测试里，Grounding DINO 既有更高的平均水平，也有<strong>更高的中位数</strong>（更稳）。</li>
<li>小模型（Swin-T）在 Full-shot 上能<strong>反超</strong>更大骨干的闭集 DINO，提示<strong>跨模态融合带来的表示质量</strong>对下游监督也有帮助。</li>
</ul>
<hr>
<h3 id="贯穿性的技术洞察（来自作者在本节的讨论）"><a href="#贯穿性的技术洞察（来自作者在本节的讨论）" class="headerlink" title="贯穿性的技术洞察（来自作者在本节的讨论）"></a>贯穿性的技术洞察（来自作者在本节的讨论）</h3><ol>
<li><p><strong>Grounding 数据的价值</strong>：在 COCO&#x2F;LVIS&#x2F;ODinW 的 zero-shot 上普遍<strong>带来&gt;1 AP</strong>的增益；</p>
</li>
<li><p><strong>Caption 数据的价值</strong>：对零样本<strong>有帮助</strong>，但增益大小依数据集而异；</p>
</li>
<li><p><strong>可扩展性与一致性</strong>：更强骨干+更大预训练库→COCO 52.5 AP（zero-shot），ODinW 26.1（zero-shot），并在 AP_median 上显示稳定性优势；</p>
</li>
<li><p><strong>局限提醒</strong>：</p>
<ul>
<li>O365→COCO 的类目映射是<strong>近似</strong>（零样本数字可能保守）；</li>
<li><strong>LVIS 稀有类</strong>仍然是 DETR 系的短板，要进一步靠数据分布匹配或专门策略缓解。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="一页速览（数字总表）"><a href="#一页速览（数字总表）" class="headerlink" title="一页速览（数字总表）"></a>一页速览（数字总表）</h3><ul>
<li><strong>COCO（zero-shot）</strong>：Swin-T <strong>46.7→48.1→48.4</strong>（O365→+GoldG→+Cap4M）；Swin-L <strong>52.5</strong>（创纪录）。</li>
<li><strong>COCO（fine-tune）</strong>：Swin-L <strong>62.6</strong>（minival）；1.5× 输入 <strong>63.0</strong>（test-dev）。</li>
<li><strong>LVIS（zero-shot, Swin-T）</strong>：<strong>25.6 → 27.4</strong>（+Cap4M）。</li>
<li><strong>LVIS（fine-tune, Swin-T）</strong>：<strong>52.1</strong>（&gt; DetCLIPv2-T 50.7）。</li>
<li><strong>ODinW（zero-shot）</strong>：Swin-T <strong>22.3</strong>（带 Cap4M），Swin-L <strong>26.1</strong>（SOTA，AP_median 18.4）；</li>
<li><strong>ODinW（few&#x2F;full-shot, Swin-T）</strong>：<strong>46.4 &#x2F; 70.7</strong>。</li>
</ul>
<hr>
<h2 id="Referring-Object-Detection-Settings"><a href="#Referring-Object-Detection-Settings" class="headerlink" title="Referring Object Detection Settings"></a>Referring Object Detection Settings</h2><h3 id="一、研究背景与动机"><a href="#一、研究背景与动机" class="headerlink" title="一、研究背景与动机"></a>一、研究背景与动机</h3><p>在第 4 章中，作者分别研究了三种任务设定：<br>1️⃣ <strong>Closed-set detection（封闭集检测）</strong><br>2️⃣ <strong>Zero-shot open-set detection（零样本开放集检测）</strong><br>3️⃣ <strong>Referring Object Detection (Referring Expression Comprehension, REC)</strong></p>
<p>第 4.3 节主要聚焦第 3 种设定，即<strong>指代表达检测任务</strong>。这类任务输入通常是一句自然语言描述（例如 “the man in a red shirt”），模型需要返回图像中对应目标的边界框。相比常规类别检测，REC 任务要求模型具有<strong>更细粒度的视觉–语言对齐能力</strong>（不仅识别“人”，还要理解描述中的属性或关系）。</p>
<hr>
<h3 id="二、实验设置与对比方法"><a href="#二、实验设置与对比方法" class="headerlink" title="二、实验设置与对比方法"></a>二、实验设置与对比方法</h3><p>作者在该小节中：</p>
<ul>
<li><strong>基线模型</strong> 选择了 GLIP [25]，因为它同样将检测任务表述为 “grounding（短语定位）” 问题。</li>
<li><strong>评测数据集</strong> 包括 RefCOCO、RefCOCO+ 和 RefCOCOg 三套标准 REC 基准（文中统称为 “RefC”）。</li>
<li><strong>评估方式</strong> ：模型不额外使用这些 REC 数据进行训练，直接测试其零样本性能，以衡量从开放集检测到指代表达理解的迁移能力。</li>
<li><strong>骨干网络</strong> 均为 ResNet-101，确保对比公平。</li>
</ul>
<hr>
<h3 id="三、主要结果（表-5-摘要）"><a href="#三、主要结果（表-5-摘要）" class="headerlink" title="三、主要结果（表 5 摘要）"></a>三、主要结果（表 5 摘要）</h3><p>表 5 报告了多个代表性 REC 模型与 Grounding DINO 的对比：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>骨干</th>
<th>预训练数据</th>
<th>是否微调</th>
<th>RefCOCO (val&#x2F;testA&#x2F;testB)</th>
<th>RefCOCO+</th>
<th>RefCOCOg</th>
</tr>
</thead>
<tbody><tr>
<td>MAttNet</td>
<td>R101</td>
<td>无</td>
<td>✓</td>
<td>76.7 &#x2F; 81.1 &#x2F; 70.0</td>
<td>65.3 &#x2F; 71.6 &#x2F; 56.0</td>
<td>66.6 &#x2F; 67.3</td>
</tr>
<tr>
<td>MDETR</td>
<td>R101</td>
<td>GoldG, RefC</td>
<td>✓</td>
<td>86.8 &#x2F; 89.6 &#x2F; 81.4</td>
<td>79.5 &#x2F; 84.1 &#x2F; 70.6</td>
<td>81.6 &#x2F; 80.9</td>
</tr>
<tr>
<td><strong>Grounding DINO (T)</strong></td>
<td>Swin-T</td>
<td>O365, GoldG (+Cap4M)</td>
<td>✗ &#x2F; ✓</td>
<td>略低于 MDETR 但优于 GLIP</td>
<td>—</td>
<td>—</td>
</tr>
</tbody></table>
<blockquote>
<p>在不使用 REC 训练数据的情况下，Grounding DINO 仍然超过 GLIP 基线；但绝对精度远低于专门针对 REC 训练的 MDETR 或 DQ-DETR。</p>
</blockquote>
<hr>
<h3 id="四、作者分析与结论"><a href="#四、作者分析与结论" class="headerlink" title="四、作者分析与结论"></a>四、作者分析与结论</h3><p>1️⃣ <strong>零样本性能有限</strong><br> – 作者指出，无论是 GLIP 还是 Grounding DINO，在未见过 REC 数据时效果都不佳。<br> – 这表明通用开放集检测模型在面对带有属性与关系的细粒度语言时仍存在泛化障碍。</p>
<p>2️⃣ <strong>训练数据与规模的影响</strong><br> – 添加 caption 数据或扩大模型规模虽有帮助，但提升幅度<strong>很小</strong>（minor）。<br> – <strong>关键突破来自于加入 RefCOCO&#x2F;+&#x2F;g 训练数据</strong>：一旦注入这些 REC 样本，模型性能显著提升，说明<strong>专门的细粒度对齐监督</strong>对指代表达理解至关重要。</p>
<p>3️⃣ <strong>开放集检测器的启示</strong><br> – 现有开放集检测方法普遍关注类别扩展，而忽视了“细粒度语言–视觉对齐”。<br> – Grounding DINO 的实验揭示：若要在 REC 等任务中真正泛化，开放集检测器必须更重视<strong>属性级与关系级别的表征学习</strong>。</p>
<hr>
<h3 id="五、延伸讨论"><a href="#五、延伸讨论" class="headerlink" title="五、延伸讨论"></a>五、延伸讨论</h3><ul>
<li><strong>理论意义</strong>：Grounding DINO 展示了同一模型架构在“类别名 → 物体”与“自然描述 → 物体”两种语义层面的统一潜力。</li>
<li><strong>实践意义</strong>：这为后续多模态应用（如 视觉问答、图文检索、基于文本的图像编辑）奠定了通用基础。</li>
<li><strong>局限与未来方向</strong>：作者在附录 D.3 中进一步说明 RefCOCO 与 grounding 数据的差异——RefCOCO 中每条文本只对应一个目标框，而 Grounding DINO 在训练中习惯输出多个候选框，这种<strong>任务定义差异</strong>是零样本 REC 性能差距的重要原因。</li>
</ul>
<hr>
<p>✅ <strong>总结一句话</strong>：</p>
<blockquote>
<p>第 4.3 节揭示了 Grounding DINO 在指代表达检测中的零样本迁移表现虽超越 GLIP，但仍受限于缺乏 REC 专属监督；注入 RefCOCO 类数据后性能显著跃升，凸显了细粒度语义对齐的重要性。</p>
</blockquote>
<h2 id="Effects-of-RefC-and-COCO-Data"><a href="#Effects-of-RefC-and-COCO-Data" class="headerlink" title="Effects of RefC and COCO Data"></a>Effects of RefC and COCO Data</h2><hr>
<h3 id="一、研究动机与目标"><a href="#一、研究动机与目标" class="headerlink" title="一、研究动机与目标"></a>一、研究动机与目标</h3><p>作者在前一节 (4.3) 中发现：</p>
<ul>
<li>没有 RefCOCO 类训练数据时，Grounding DINO 和 GLIP 在 Referring Detection 任务上表现都不好。</li>
<li>一旦引入 RefC 数据，性能显著上升。</li>
</ul>
<p>因此在 <strong>4.4 节</strong>，作者进一步量化这类数据对整个模型（包括 COCO、LVIS 和 ODinW 等开放集任务）的全局影响，试图回答三个问题：</p>
<p>1️⃣ <strong>加入 RefCOCO 系列 (RefC)</strong> 是否会提升 Grounding DINO 的整体泛化能力？<br>2️⃣ <strong>是否会出现任务间负迁移</strong>（即 RefC 提升 REC 但损伤其他检测任务）？<br>3️⃣ <strong>COCO 检测数据</strong>（虽然常用于闭集训练）对开放集性能有何影响？</p>
<hr>
<h3 id="二、实验设置"><a href="#二、实验设置" class="headerlink" title="二、实验设置"></a>二、实验设置</h3><p>作者基于 <strong>Swin-T</strong> 骨干，分别测试三种预训练组合：</p>
<table>
<thead>
<tr>
<th align="left">模型</th>
<th align="left">预训练数据</th>
<th align="center">COCO (minival)</th>
<th align="center">LVIS (minival)</th>
<th align="center">ODinW</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Grounding DINO T</td>
<td align="left">O365 + GoldG</td>
<td align="center">48.1 (zero-shot) &#x2F; 57.1 (fine-tune)</td>
<td align="center">25.6</td>
<td align="center">20.0</td>
</tr>
<tr>
<td align="left">Grounding DINO T</td>
<td align="left">O365 + GoldG + RefC</td>
<td align="center">48.5 &#x2F; 57.3</td>
<td align="center">21.9</td>
<td align="center">17.7</td>
</tr>
<tr>
<td align="left">Grounding DINO T</td>
<td align="left">O365 + GoldG + RefC + COCO</td>
<td align="center">56.1 &#x2F; 57.5</td>
<td align="center">22.3</td>
<td align="center">17.4</td>
</tr>
</tbody></table>
<blockquote>
<p>其中 “RefC” 指 RefCOCO、RefCOCO+、RefCOCOg 的统称。<br>Zero-Shot 表示完全未在目标数据上训练；Fine-Tune 表示在该数据上微调后结果。</p>
</blockquote>
<hr>
<h3 id="三、结果分析"><a href="#三、结果分析" class="headerlink" title="三、结果分析"></a>三、结果分析</h3><p>1️⃣ RefC 数据的影响</p>
<ul>
<li><p><strong>在 COCO 上</strong>：<br>零样本 AP 从 48.1 → 48.5 （微弱 +0.4），几乎无明显提升。</p>
</li>
<li><p><strong>在 LVIS 上</strong>：<br>从 25.6 → 21.9 ，性能<strong>显著下降 (−3.7 AP)</strong>。说明 RefC 训练可能破坏了长尾分布类的检测泛化。</p>
</li>
<li><p><strong>在 ODinW 上</strong>：<br>从 20.0 → 17.7 ，下降 2.3 AP。显示 RefC 的语言特征偏向描述性句式，与 ODinW 的简短类名检测目标不匹配，导致负迁移。</p>
</li>
</ul>
<p>👉 <strong>结论</strong>：RefC 能提升 REC 任务，但会降低 LVIS 和 ODinW 等开放集检测性能。</p>
<hr>
<p>2️⃣ COCO 数据的影响</p>
<ul>
<li><p>在加入 COCO 后（即 O365 + GoldG + RefC + COCO ）：</p>
<ul>
<li><strong>COCO zero-shot 显著上升</strong> 至 56.1 AP （大 幅 +7.6 AP），</li>
<li><strong>LVIS 微升</strong> 至 22.3 (+0.4)，</li>
<li><strong>ODinW 略降</strong> 至 17.4 (−0.3)。</li>
</ul>
</li>
</ul>
<p>👉 说明 COCO 训练主要提升与其域相近的 COCO 检测性能，对 LVIS 和 ODinW 帮助有限甚至略有冲突。</p>
<hr>
<h3 id="四、作者总结与洞察"><a href="#四、作者总结与洞察" class="headerlink" title="四、作者总结与洞察"></a>四、作者总结与洞察</h3><p>1️⃣ <strong>RefC → 细粒度描述增强</strong><br> RefC 可强化模型理解自然语言中对象属性与关系的能力，因此对 Referring 任务有益；<br> 但因其句式不同于单词级 category 标签，会削弱模型在简短 class name 检测时的泛化。</p>
<p>2️⃣ <strong>COCO → 同域强化</strong><br> COCO 训练显著提高 COCO 本身的 AP，说明数据分布相似性仍然是决定泛化的重要因素。</p>
<p>3️⃣ <strong>多任务数据的平衡问题</strong><br> Grounding DINO 在不同数据域（检测 vs. 描述）之间存在“trade-off”；<br> 如何在不牺牲 ODinW 与 LVIS 性能的前提下引入 RefC 类细粒度数据，是未来关键方向。</p>
<p>4️⃣ <strong>隐含结论</strong><br> RefC 与 COCO 在语言风格和目标定义上差异较大。RefC 强调<strong>自然语言指称</strong>，COCO 强调<strong>固定类标</strong>；<br> 混合使用时会带来<strong>领域偏移 (domain shift)</strong>，影响跨域泛化。</p>
<hr>
<h3 id="五、可视化理解"><a href="#五、可视化理解" class="headerlink" title="五、可视化理解"></a>五、可视化理解</h3><ul>
<li>RefC 文本： “the man in a blue shirt” → 对应 1 个 box。</li>
<li>COCO &#x2F; ODinW 文本： “person” → 对应 多个 box。</li>
</ul>
<p>Grounding DINO 在 RefC 训练时学到“一句描述对应一个 box”的规律，因此在开放集检测（多目标预测）时更容易“收敛过度”，造成 AP 下降。</p>
<hr>
<h3 id="✅-总结一句话"><a href="#✅-总结一句话" class="headerlink" title="✅ 总结一句话"></a>✅ 总结一句话</h3><blockquote>
<p><strong>4.4 节揭示了数据源对开放集检测泛化的关键影响：</strong></p>
<ul>
<li>RefCOCO 系列（RefC）能增强 Referring 类任务但会损害跨域检测性能；</li>
<li>COCO 数据显著提升 COCO 域表现，却对 LVIS 和 ODinW 提升有限。<br>作者强调未来应在<strong>跨数据类型的协同训练与语言风格对齐</strong>上继续探索，以兼顾 fine-grained 理解与泛化能力。</li>
</ul>
</blockquote>
<h2 id="Ablations"><a href="#Ablations" class="headerlink" title="Ablations"></a>Ablations</h2><hr>
<h3 id="一、研究目的"><a href="#一、研究目的" class="headerlink" title="一、研究目的"></a>一、研究目的</h3><p>Grounding DINO 的主要创新点包括：<br>1️⃣ <strong>多阶段紧耦合（Tight Fusion）跨模态设计</strong>：<br> 即在 encoder–decoder 的不同阶段都引入语言–视觉交互，包括：</p>
<p> - 特征增强（Feature Enhancer）阶段<br> - 语言引导查询选择（Language-guided Query Selection）阶段<br> - 解码阶段的跨模态注意力（Cross-Modality Decoder）</p>
<p>2️⃣ <strong>子句级文本表示（Sub-sentence Text Prompt）</strong>：<br> 在文本嵌入时，通过掩蔽不相关类别间的注意力（attention mask），减少不同类别之间的干扰。</p>
<p>作者在 4.5 节中要回答两个问题：</p>
<ul>
<li>哪些模块对性能贡献最大？</li>
<li>哪些改进对零样本（zero-shot）与微调（fine-tune）效果影响不同？</li>
</ul>
<hr>
<h3 id="二、实验设置-1"><a href="#二、实验设置-1" class="headerlink" title="二、实验设置"></a>二、实验设置</h3><ul>
<li><p><strong>模型规模</strong>：均为 <strong>Grounding DINO-T (Swin-T backbone)</strong></p>
</li>
<li><p><strong>预训练数据</strong>：仅用 <strong>Objects365 (O365)</strong></p>
</li>
<li><p><strong>评测集</strong>：</p>
<ul>
<li>COCO（minival）：测试 Zero-Shot 与 Fine-Tune</li>
<li>LVIS（minival）：测试 Zero-Shot</li>
</ul>
</li>
</ul>
<p>作者定义了五个实验变体，如下表（Table 7）：</p>
<table>
<thead>
<tr>
<th align="left">#</th>
<th align="left">模型变体</th>
<th align="center">COCO (Zero-shot)</th>
<th align="center">COCO (Fine-tune)</th>
<th align="center">LVIS (Zero-shot)</th>
</tr>
</thead>
<tbody><tr>
<td align="left">0</td>
<td align="left">Full Model (完整模型)</td>
<td align="center">46.7</td>
<td align="center">56.9</td>
<td align="center">16.1</td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">w&#x2F;o encoder fusion（无编码器级融合）</td>
<td align="center">45.8</td>
<td align="center">56.1</td>
<td align="center">13.1</td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">static query selection（静态查询选择）</td>
<td align="center">46.3</td>
<td align="center">56.6</td>
<td align="center">13.6</td>
</tr>
<tr>
<td align="left">3</td>
<td align="left">w&#x2F;o text cross-attention（无文本交叉注意力）</td>
<td align="center">46.1</td>
<td align="center">56.3</td>
<td align="center">14.3</td>
</tr>
<tr>
<td align="left">4</td>
<td align="left">word-level text prompt（词级文本表示）</td>
<td align="center">46.4</td>
<td align="center">56.6</td>
<td align="center">15.6</td>
</tr>
</tbody></table>
<hr>
<h3 id="三、结果与分析"><a href="#三、结果与分析" class="headerlink" title="三、结果与分析"></a>三、结果与分析</h3><p>1️⃣ 编码器融合（Encoder Fusion）的影响最大</p>
<ul>
<li>去掉该模块后（#1），<strong>COCO zero-shot 降 0.9，LVIS zero-shot 降 3.0 AP</strong>。</li>
<li>说明在视觉编码阶段加入语言–图像融合，对开放集检测的泛化至关重要。</li>
</ul>
<blockquote>
<p>这是模型最核心的创新之一，支撑了其在多任务中的优异表现。</p>
</blockquote>
<hr>
<p>2️⃣ 语言引导查询选择（Language-Guided Query Selection）</p>
<ul>
<li>将动态选择改为静态（#2）后，COCO zero-shot 降 0.4，LVIS 降 2.5。</li>
<li>表明“使用文本内容动态选择最相关的视觉查询”确实提升模型对语言指向性的敏感度。</li>
</ul>
<hr>
<p>3️⃣ 文本交叉注意力（Text Cross-Attention）</p>
<ul>
<li>移除后（#3），COCO zero-shot 降 0.6，LVIS 降 1.8。</li>
<li>虽然引入参数较少，但依然能提升模型跨模态理解。</li>
<li>作者指出：<strong>Fine-tune 阶段改进有限</strong>，说明该模块对零样本泛化作用更显著。</li>
</ul>
<hr>
<p>4️⃣ 子句级文本提示（Sub-sentence Text Prompt）</p>
<ul>
<li>改回词级输入（#4）后，LVIS zero-shot 降 0.5 AP。</li>
<li>表明子句级表示可有效减少不同类词间的干扰，尤其在类目多、语义复杂的场景中表现更稳定。</li>
</ul>
<hr>
<p>5️⃣ 综合结论</p>
<ul>
<li>在 Zero-shot 场景下，所有模块都带来不同程度提升；</li>
<li>在 Fine-tune 场景下，性能差距明显缩小，说明语言结构模块主要影响<strong>跨域泛化</strong>，而非闭集优化。</li>
<li><strong>Scaling（放大模型）</strong> 仍是提升 Fine-tune 性能的关键途径。</li>
</ul>
<hr>
<h3 id="四、作者进一步讨论"><a href="#四、作者进一步讨论" class="headerlink" title="四、作者进一步讨论"></a>四、作者进一步讨论</h3><p>1️⃣ <strong>Encoder Fusion 的核心地位</strong><br> 作者强调这是最关键的跨模态融合阶段，决定了图文特征是否充分对齐。</p>
<p>2️⃣ <strong>模块轻量但有效</strong><br> Language-guided query 与 sub-sentence prompt 都<strong>不增加参数量或计算开销</strong>，但显著提升开放集性能。</p>
<p>3️⃣ <strong>模型规模与性能关系</strong><br> Fine-tune 后性能受模型规模主导，而非模块设计；<br> 因此更大骨干（如 Swin-L）是未来提升方向。</p>
<hr>
<h3 id="五、总体结论（作者原文总结）"><a href="#五、总体结论（作者原文总结）" class="headerlink" title="五、总体结论（作者原文总结）"></a>五、总体结论（作者原文总结）</h3><blockquote>
<p>“Encoder fusion significantly improves model performance on both COCO and LVIS datasets… Other techniques such as language-guided query selection, text cross-attention, and sub-sentence text prompt also contribute positively, particularly on LVIS. These methods enhance zero-shot performance, underscoring their effectiveness.”<br>（编码器融合显著提升 COCO 与 LVIS 性能；语言引导查询、文本交叉注意力、子句级文本提示同样带来增益，尤其在 LVIS 上。这些改进主要增强了零样本表现，验证了设计的有效性。）</p>
</blockquote>
<hr>
<h3 id="✅-小结一句话"><a href="#✅-小结一句话" class="headerlink" title="✅ 小结一句话"></a>✅ 小结一句话</h3><blockquote>
<p><strong>第 4.5 节表明：</strong> Grounding DINO 的性能提升来源于系统性的跨模态融合，而非单一技巧。<br>其中最关键的是 <strong>Encoder Fusion</strong>；<br>而 <strong>Language-guided Query Selection</strong>、<strong>Text Cross-Attention</strong> 与 <strong>Sub-sentence Text Prompt</strong> 则是轻量但对零样本泛化极为重要的补充机制。</p>
</blockquote>
<hr>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><hr>
<h2 id="一、总体定位：让-DINO-拥抱开放世界"><a href="#一、总体定位：让-DINO-拥抱开放世界" class="headerlink" title="一、总体定位：让 DINO 拥抱开放世界"></a>一、总体定位：让 DINO 拥抱开放世界</h2><p>作者在开头重申了他们的主要目标：</p>
<blockquote>
<p><strong>“We have presented a Grounding DINO model in this paper. Grounding DINO extends DINO to open-set object detection, enabling it to detect arbitrary objects given texts as queries.”</strong></p>
</blockquote>
<p>也就是说，Grounding DINO 的核心使命是：<br>把原本**只会识别预定义类别（closed-set）<strong>的 DINO，扩展为</strong>能理解任意语言输入（open-set）**的检测器。<br>换句话说，传统 DINO 只能识别比如 “cat &#x2F; dog &#x2F; car”，而 Grounding DINO 可以识别</p>
<blockquote>
<p>“the man in blue”, “the tall building on the right”, “any novel object described in words”。</p>
</blockquote>
<p>这一点实现了「从分类器到语言理解者」的跨越。</p>
<hr>
<h2 id="二、主要创新点总结"><a href="#二、主要创新点总结" class="headerlink" title="二、主要创新点总结"></a>二、主要创新点总结</h2><p>作者紧接着总结了本文的三项关键创新设计：</p>
<h3 id="1️⃣-Tight-Fusion-跨模态融合架构"><a href="#1️⃣-Tight-Fusion-跨模态融合架构" class="headerlink" title="1️⃣ Tight Fusion 跨模态融合架构"></a>1️⃣ <strong>Tight Fusion 跨模态融合架构</strong></h3><p>他们提出了一个“紧耦合的（tight fusion）”视觉–语言融合方案，贯穿整个 DINO 架构的三个阶段：</p>
<ul>
<li><strong>Encoder</strong>：引入 image-to-text 与 text-to-image 双向 cross-attention；</li>
<li><strong>Query Initialization</strong>：通过 <strong>Language-guided Query Selection</strong> 选出与文本语义最相关的视觉查询；</li>
<li><strong>Decoder</strong>：采用 cross-modality decoder 联合细化 box 与语义。</li>
</ul>
<blockquote>
<p>这个设计使得语言与图像在不同层面充分交互，从而在开放集检测任务中具备强泛化能力。</p>
</blockquote>
<hr>
<h3 id="2️⃣-Sub-sentence-Level-Text-Representation"><a href="#2️⃣-Sub-sentence-Level-Text-Representation" class="headerlink" title="2️⃣ Sub-sentence Level Text Representation"></a>2️⃣ <strong>Sub-sentence Level Text Representation</strong></h3><p>他们提出了“<strong>子句级文本提示</strong>”机制，<br>通过掩蔽不相关类别间的注意力，使模型能在处理多类别句子时避免语义干扰。</p>
<p>例如在输入句子 “cat. person. mouse.” 时，<br>传统 word-level 表示会让 “cat” 与 “mouse” 的 embedding 互相影响，<br>而 sub-sentence 表示则能让模型分别理解每个词组独立的语义区域。</p>
<blockquote>
<p>作者认为，这一改进让检测数据中的文本提示（text prompts）更合理地反映语言结构，有助于细粒度理解。</p>
</blockquote>
<hr>
<h3 id="3️⃣-Open-set-Extension-to-Referring-Expression-Comprehension-REC"><a href="#3️⃣-Open-set-Extension-to-Referring-Expression-Comprehension-REC" class="headerlink" title="3️⃣ Open-set Extension to Referring Expression Comprehension (REC)"></a>3️⃣ <strong>Open-set Extension to Referring Expression Comprehension (REC)</strong></h3><p>他们首次<strong>把开放集检测模型扩展到 Referring Object Detection（指代表达检测）</strong>，<br>并系统评估了模型在该任务下的表现。</p>
<p>实验显示：</p>
<ul>
<li>现有的开放集检测器（GLIP、Grounding DINO 等）<strong>在零样本 REC 上表现都不理想</strong>；</li>
<li>这表明开放集模型仍需提升对细粒度语言描述（属性、关系）的理解。</li>
</ul>
<blockquote>
<p>因此，他们的工作也为今后的“视觉–语言统一检测模型”提供了可复用框架。</p>
</blockquote>
<hr>
<h2 id="三、关键实验结论"><a href="#三、关键实验结论" class="headerlink" title="三、关键实验结论"></a>三、关键实验结论</h2><p>作者在结尾再次强调实验验证的主要发现：</p>
<ul>
<li><p>他们的<strong>融合结构</strong>和<strong>子句级文本建模</strong>均有效提升了性能；</p>
</li>
<li><p>特别是在 <strong>COCO 与 LVIS</strong> 的 Zero-shot 场景中，</p>
<ul>
<li>Encoder Fusion 提升最大；</li>
<li>Language-guided Query Selection、Text Cross-Attention、Sub-sentence Prompt 也带来显著增益；</li>
</ul>
</li>
<li><p>即使 Fine-tune 后（闭集训练），模型仍能保持高水平性能。</p>
</li>
</ul>
<p>这说明 Grounding DINO 不仅在开放集泛化上强，同时在标准检测任务中也具备竞争力。</p>
<hr>
<h2 id="四、整体贡献与意义"><a href="#四、整体贡献与意义" class="headerlink" title="四、整体贡献与意义"></a>四、整体贡献与意义</h2><p><strong>Grounding DINO 的价值，可以概括为三点：</strong></p>
<table>
<thead>
<tr>
<th>贡献方向</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>🧠 模型层面</td>
<td>实现了 Transformer 检测器与 grounded 语言预训练的统一；提出了全阶段融合机制；</td>
</tr>
<tr>
<td>📊 实验层面</td>
<td>刷新了多个基准：COCO Zero-shot (52.5 AP)、ODinW Zero-shot (26.1 AP)；</td>
</tr>
<tr>
<td>🌍 应用层面</td>
<td>展示了在 Referring Detection 与开放词汇检测上的广泛潜力；还可与 Stable Diffusion 结合进行“可控生成”。</td>
</tr>
</tbody></table>
<p>作者也强调：<br>他们的研究表明，<strong>“open-set object detection” 不再仅仅是类别扩展问题，而是一个视觉–语言对齐问题。</strong><br>通过强耦合的融合结构与合适的语言建模，模型才能真正理解“任意人类输入的描述”。</p>
<hr>
<h2 id="五、未来展望（隐含在结论中的启示）"><a href="#五、未来展望（隐含在结论中的启示）" class="headerlink" title="五、未来展望（隐含在结论中的启示）"></a>五、未来展望（隐含在结论中的启示）</h2><p>虽然作者在结论中未单独列出“Future Work”小节，但可以从语气中读出几条潜在方向：<br>1️⃣ <strong>继续扩展模型规模与数据多样性</strong>（例如结合更多 caption、grounding 数据集）；<br>2️⃣ <strong>加强细粒度语义理解</strong>，解决指代表达（REC）任务的性能瓶颈；<br>3️⃣ <strong>融合生成模型</strong>（文中提及结合 Stable Diffusion、GLIGEN 的应用潜力）；<br>4️⃣ <strong>探索跨任务统一训练</strong>，使模型在 detection、segmentation、grounding 等任务间无缝迁移。</p>
<hr>
<h2 id="✅-总结一句话-1"><a href="#✅-总结一句话-1" class="headerlink" title="✅ 总结一句话"></a>✅ 总结一句话</h2><blockquote>
<p><strong>Grounding DINO 的结论核心是：</strong><br>它成功将 DINO 从闭集检测扩展为一个可理解自然语言指令的开放集检测器；<br>通过「紧耦合跨模态融合 + 子句级文本提示」，实现了显著的零样本泛化性能，<br>并首次在开放集检测与指代表达理解之间建立了统一框架。</p>
</blockquote>
<hr>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/blog/tags/%E9%9B%B6%E6%A0%B7%E6%9C%AC%E6%A3%80%E6%B5%8B/">零样本检测</a></div><div class="post-share"><div class="social-share" data-image="/blog/image/A1-1.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/blog/2025/10/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AZero-Shot-Detection/" title="论文阅读：Zero-Shot Detection"><img class="cover" src="/blog/image/62aa03063f117eaad7c77592e3b98d7f05b0a86329e44a-TuUO3E.jpg" onerror="onerror=null;src='/blog/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">论文阅读：Zero-Shot Detection</div></div><div class="info-2"><div class="info-item-1">INTRODUCTION1）动机：检测要走向“长尾、开放世界” 在大规模应用里（想象自动驾驶、通用机器人、视频理解），不可能为每一个可能的目标类别都收集充足的“框+类别”标注。 传统检测（如 YOLOv2、Faster R-CNN）需要大量带框监督，这在规模化时不可持续。因此，研究界从零样本学习（ZSL）里借力：用语义（属性词、词向量、文本描述等）把“没见过的类”与“见过的类”连接起来，从而在训练时没见过某些类别、测试时要识别检测它们。  不过，过往 ZSL 多是“分类”问题（图像里物体已被很好地裁出来，只需认类）。现实却是更难的“检测”：不仅要认，还要找（定位边界框）。这正是本文定义并要解决的零样本检测（ZSD）。   2）传统检测器在“未见类”上为什么会失手？ 以 YOLOv2 为例，性能高的一个关键是：它在训练中学到一套非常判别的视觉特征，并通过“目标性（objectness）置信度”的损失把背景强力压下，只保留与“见过的类”相似的候选框。...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/blog/2025/09/23/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" title="小样本目标检测"><img class="cover" src="/blog/image/29051e0d6c0e82fe7e46a7e50399ff577917a1a3c82ee-6Qx5wo.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-23</div><div class="info-item-2">小样本目标检测</div></div><div class="info-2"><div class="info-item-1">名词概念 少样本对象检测 (Few-Shot Object Detection, FSOD): 每个新类别提供少量（比如1到10个）标注样本 。这是最主流的研究方向。  单样本对象定位 (One-Shot Object Localization, OSOL): 这是 FSOD 的一个特例，每个新类别只提供一个标注样本 。  零样本对象检测 (Zero-Shot Object Detection, ZSOD): 这是最极端的情况，新类别没有任何标注的图像样本 。那模型怎么学呢？它依赖于额外的信息，比如描述这些类别的语义属性（例如，描述“斑马”的词是“条纹”、“像马的动物”）   key words深度学习在低样本目标检测中的综述 A Survey of Deep Learning for Low-Shot Object Detection Additional Key Words and Phrases: Few-Shot Object Detection, One-Shot Object Detection, Zero-Shot Object detection,...</div></div></div></a><a class="pagination-related" href="/blog/2025/10/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AZero-Shot-Detection/" title="论文阅读：Zero-Shot Detection"><img class="cover" src="/blog/image/62aa03063f117eaad7c77592e3b98d7f05b0a86329e44a-TuUO3E.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-20</div><div class="info-item-2">论文阅读：Zero-Shot Detection</div></div><div class="info-2"><div class="info-item-1">INTRODUCTION1）动机：检测要走向“长尾、开放世界” 在大规模应用里（想象自动驾驶、通用机器人、视频理解），不可能为每一个可能的目标类别都收集充足的“框+类别”标注。 传统检测（如 YOLOv2、Faster R-CNN）需要大量带框监督，这在规模化时不可持续。因此，研究界从零样本学习（ZSL）里借力：用语义（属性词、词向量、文本描述等）把“没见过的类”与“见过的类”连接起来，从而在训练时没见过某些类别、测试时要识别检测它们。  不过，过往 ZSL 多是“分类”问题（图像里物体已被很好地裁出来，只需认类）。现实却是更难的“检测”：不仅要认，还要找（定位边界框）。这正是本文定义并要解决的零样本检测（ZSD）。   2）传统检测器在“未见类”上为什么会失手？ 以 YOLOv2 为例，性能高的一个关键是：它在训练中学到一套非常判别的视觉特征，并通过“目标性（objectness）置信度”的损失把背景强力压下，只保留与“见过的类”相似的候选框。...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/blog/image/IMG_20250131_155849.jpg" onerror="this.onerror=null;this.src='/blog/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">lian</div><div class="author-info-description">太平山上修真我，祖师堂中续香火</div><div class="site-data"><a href="/blog/archives/"><div class="headline">文章</div><div class="length-num">29</div></a><a href="/blog/tags/"><div class="headline">标签</div><div class="length-num">9</div></a><a href="/blog/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="mailto:2895014608@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">QQ-2895014608</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Introduction"><span class="toc-number">1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Related-Work"><span class="toc-number">2.</span> <span class="toc-text">Related Work</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8E-DETR-%E7%B3%BB%E5%88%B0-DINO%EF%BC%9A%E6%A3%80%E6%B5%8B%E5%99%A8%E4%B8%BB%E7%BA%BF%EF%BC%88%E9%97%AD%E9%9B%86%E8%83%8C%E6%99%AF%EF%BC%89"><span class="toc-number">2.1.</span> <span class="toc-text">从 DETR 系到 DINO：检测器主线（闭集背景）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%80%E6%94%BE%E9%9B%86%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%9A%E4%B8%A4%E6%9D%A1%E4%BB%A3%E8%A1%A8%E6%80%A7%E8%B7%AF%E7%BA%BF"><span class="toc-number">2.2.</span> <span class="toc-text">开放集目标检测：两条代表性路线</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E8%87%AA%E5%AE%9A%E4%BD%8D%EF%BC%9A%E4%B8%89%E7%9B%B8%E4%BD%8D%E8%9E%8D%E5%90%88-%E6%9B%B4%E7%BB%86%E7%B2%92%E5%BA%A6%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA"><span class="toc-number">2.3.</span> <span class="toc-text">论文自定位：三相位融合 + 更细粒度文本表示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Table-1"><span class="toc-number">2.4.</span> <span class="toc-text">Table 1</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93%EF%BC%9ARelated-Work-%E7%BB%99%E6%9C%AC%E6%96%87%E2%80%9C%E6%90%AD%E4%BA%86%E5%93%AA%E4%BA%9B%E5%8F%B0%E5%AD%90%E2%80%9D"><span class="toc-number">2.5.</span> <span class="toc-text">小结：Related Work 给本文“搭了哪些台子”</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Grounding-DINO"><span class="toc-number">3.</span> <span class="toc-text">Grounding DINO</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%A7%A9-Feature-Extraction-and-Enhancer"><span class="toc-number">3.1.</span> <span class="toc-text">🧩  Feature Extraction and Enhancer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%A7%AD-Language-Guided-Query-Selection"><span class="toc-number">3.2.</span> <span class="toc-text">🧭 Language-Guided Query Selection</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cross-Modality-Decoder"><span class="toc-number">3.3.</span> <span class="toc-text">Cross-Modality Decoder</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Sub-Sentence-Level-Text-Feature"><span class="toc-number">3.4.</span> <span class="toc-text">Sub-Sentence Level Text Feature</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%A7%AD-%E4%B8%80%E3%80%81%E8%83%8C%E6%99%AF%E9%97%AE%E9%A2%98%EF%BC%9A%E8%AF%AD%E8%A8%80%E7%89%B9%E5%BE%81%E8%A1%A8%E7%A4%BA%E7%9A%84%E4%B8%A4%E7%A7%8D%E5%B8%B8%E8%A7%81%E6%96%B9%E5%BC%8F"><span class="toc-number">3.4.1.</span> <span class="toc-text">🧭 一、背景问题：语言特征表示的两种常见方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%A7%A9-%E4%BA%8C%E3%80%81%E9%97%AE%E9%A2%98%E6%9C%AC%E8%B4%A8%EF%BC%9A%E7%B1%BB%E5%88%AB%E9%97%B4%E7%9A%84%E2%80%9C%E5%B9%B2%E6%89%B0%E6%B3%A8%E6%84%8F%E5%8A%9B%E2%80%9D"><span class="toc-number">3.4.2.</span> <span class="toc-text">🧩 二、问题本质：类别间的“干扰注意力”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%92%A1-%E4%B8%89%E3%80%81Grounding-DINO-%E7%9A%84%E5%88%9B%E6%96%B0%EF%BC%9ASub-Sentence-Level-Representation%EF%BC%88%E5%AD%90%E5%8F%A5%E7%BA%A7%E8%A1%A8%E7%A4%BA%EF%BC%89"><span class="toc-number">3.4.3.</span> <span class="toc-text">💡 三、Grounding DINO 的创新：Sub-Sentence Level Representation（子句级表示）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%9A%99%EF%B8%8F-%E5%9B%9B%E3%80%81%E5%AE%9E%E9%99%85%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F"><span class="toc-number">3.4.4.</span> <span class="toc-text">⚙️ 四、实际实现方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%A7%AE-%E4%BA%94%E3%80%81%E5%AF%B9%E6%AF%94%E4%B8%89%E7%A7%8D%E8%A1%A8%E7%A4%BA%E6%96%B9%E5%BC%8F%EF%BC%88%E8%A7%81-Fig-4-Page-6%EF%BC%89"><span class="toc-number">3.4.5.</span> <span class="toc-text">🧮 五、对比三种表示方式（见 Fig. 4, Page 6）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%A7%A0-%E5%85%AD%E3%80%81%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E5%AF%B9%E5%BC%80%E6%94%BE%E9%9B%86%E6%A3%80%E6%B5%8B%E5%B0%A4%E5%85%B6%E9%87%8D%E8%A6%81%EF%BC%9F"><span class="toc-number">3.4.6.</span> <span class="toc-text">🧠 六、为什么这对开放集检测尤其重要？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%88-%E4%B8%83%E3%80%81%E6%95%88%E6%9E%9C%E9%AA%8C%E8%AF%81"><span class="toc-number">3.4.7.</span> <span class="toc-text">📈 七、效果验证</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%8E%93-%E5%85%AB%E3%80%81%E6%80%BB%E7%BB%93%E4%B8%8E%E6%95%99%E5%AD%A6%E8%A7%86%E8%A7%92%E8%A7%A3%E8%AF%BB"><span class="toc-number">3.4.8.</span> <span class="toc-text">🎓 八、总结与教学视角解读</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Loss-Function"><span class="toc-number">3.5.</span> <span class="toc-text">Loss Function</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%A7%AD-%E4%B8%80%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E6%80%BB%E4%BD%93%E7%9B%AE%E6%A0%87"><span class="toc-number">3.5.1.</span> <span class="toc-text">🧭 一、损失函数的总体目标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%A7%A9-%E4%BA%8C%E3%80%81%E4%BD%8D%E7%BD%AE%E6%8D%9F%E5%A4%B1%EF%BC%88Localization-Loss%EF%BC%89"><span class="toc-number">3.5.2.</span> <span class="toc-text">🧩 二、位置损失（Localization Loss）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%94%91-%E4%B8%89%E3%80%81%E7%B1%BB%E5%88%AB%E6%8D%9F%E5%A4%B1%EF%BC%88Classification-Loss%EF%BC%89"><span class="toc-number">3.5.3.</span> <span class="toc-text">🔑 三、类别损失（Classification Loss）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%A7%A0-%E5%9B%9B%E3%80%81%E5%AF%B9%E6%AF%94%E6%8D%9F%E5%A4%B1%EF%BC%88Contrastive-Loss%EF%BC%89"><span class="toc-number">3.5.4.</span> <span class="toc-text">🧠 四、对比损失（Contrastive Loss）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%92%A1-%E4%BA%94%E3%80%81%E8%BE%85%E5%8A%A9%E6%8D%9F%E5%A4%B1%EF%BC%88Auxiliary-Loss%EF%BC%89"><span class="toc-number">3.5.5.</span> <span class="toc-text">💡 五、辅助损失（Auxiliary Loss）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%8A-%E5%85%AD%E3%80%81%E6%9C%80%E7%BB%88%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">3.5.6.</span> <span class="toc-text">📊 六、最终损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%A7%A0-%E4%B8%83%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%AE%BE%E8%AE%A1%E7%9A%84%E6%84%8F%E4%B9%89"><span class="toc-number">3.5.7.</span> <span class="toc-text">🧠 七、损失函数设计的意义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%93%9D-%E5%85%AB%E3%80%81%E6%80%BB%E7%BB%93"><span class="toc-number">3.5.8.</span> <span class="toc-text">📝 八、总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Implementation-Details"><span class="toc-number">3.6.</span> <span class="toc-text">Implementation Details</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%A7%A9-%E4%B8%80%E3%80%81%E6%80%BB%E4%BD%93%E7%BB%93%E6%9E%84%E5%9B%9E%E9%A1%BE"><span class="toc-number">3.6.1.</span> <span class="toc-text">🧩 一、总体结构回顾</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%9A%99%EF%B8%8F-%E4%BA%8C%E3%80%81%E9%AA%A8%E5%B9%B2%E7%BD%91%E7%BB%9C%E4%B8%8E%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E9%85%8D%E7%BD%AE"><span class="toc-number">3.6.2.</span> <span class="toc-text">⚙️ 二、骨干网络与特征提取配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%A7%AE-%E4%B8%89%E3%80%81%E8%BE%93%E5%85%A5%E4%B8%8E%E9%A2%84%E5%A4%84%E7%90%86%E8%AE%BE%E7%BD%AE"><span class="toc-number">3.6.3.</span> <span class="toc-text">🧮 三、输入与预处理设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%9A%80-%E5%9B%9B%E3%80%81%E8%AE%AD%E7%BB%83%E9%85%8D%E7%BD%AE%EF%BC%88Training-Configuration%EF%BC%89"><span class="toc-number">3.6.4.</span> <span class="toc-text">🚀 四、训练配置（Training Configuration）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%A7%A0-%E4%BA%94%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E5%8C%B9%E9%85%8D%E7%AD%96%E7%95%A5"><span class="toc-number">3.6.5.</span> <span class="toc-text">🧠 五、损失函数与匹配策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%A7%B0-%E5%85%AD%E3%80%81%E9%A2%84%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%BE%AE%E8%B0%83%EF%BC%88Pre-training-and-Fine-tuning%EF%BC%89"><span class="toc-number">3.6.6.</span> <span class="toc-text">🧰 六、预训练与微调（Pre-training and Fine-tuning）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%A7%AA-%E4%B8%83%E3%80%81%E6%8E%A8%E7%90%86%E9%98%B6%E6%AE%B5%EF%BC%88Inference-Details%EF%BC%89"><span class="toc-number">3.6.7.</span> <span class="toc-text">🧪 七、推理阶段（Inference Details）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%92%AC-%E5%85%AB%E3%80%81%E5%AE%9E%E7%8E%B0%E6%A1%86%E6%9E%B6%E4%B8%8E%E5%BC%80%E6%BA%90%E7%BB%86%E8%8A%82"><span class="toc-number">3.6.8.</span> <span class="toc-text">💬 八、实现框架与开源细节</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%F0%9F%8E%93-%E4%B9%9D%E3%80%81%E5%B0%8F%E7%BB%93%E4%B8%8E%E7%90%86%E8%A7%A3%E5%BB%B6%E4%BC%B8"><span class="toc-number">3.6.9.</span> <span class="toc-text">🎓 九、小结与理解延伸</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Zero-Shot-Transfer-of-Grounding-DINO"><span class="toc-number">3.7.</span> <span class="toc-text">Zero-Shot Transfer of Grounding DINO</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AC%E8%8A%82%E7%A0%94%E7%A9%B6%E8%AE%BE%E5%AE%9A%E4%B8%8E%E7%9B%AE%E7%9A%84"><span class="toc-number">3.7.1.</span> <span class="toc-text">本节研究设定与目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#COCO-%E5%9F%BA%E5%87%86%EF%BC%88%E8%A1%A8-2%EF%BC%89"><span class="toc-number">3.7.2.</span> <span class="toc-text">COCO 基准（表 2）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LVIS-%E5%9F%BA%E5%87%86%EF%BC%88%E8%A1%A8-3%EF%BC%89"><span class="toc-number">3.7.3.</span> <span class="toc-text">LVIS 基准（表 3）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ODinW%EF%BC%88Object-Detection-in-the-Wild%EF%BC%89%E5%9F%BA%E5%87%86%EF%BC%88%E8%A1%A8-4%EF%BC%89"><span class="toc-number">3.7.4.</span> <span class="toc-text">ODinW（Object Detection in the Wild）基准（表 4）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%AF%E7%A9%BF%E6%80%A7%E7%9A%84%E6%8A%80%E6%9C%AF%E6%B4%9E%E5%AF%9F%EF%BC%88%E6%9D%A5%E8%87%AA%E4%BD%9C%E8%80%85%E5%9C%A8%E6%9C%AC%E8%8A%82%E7%9A%84%E8%AE%A8%E8%AE%BA%EF%BC%89"><span class="toc-number">3.7.5.</span> <span class="toc-text">贯穿性的技术洞察（来自作者在本节的讨论）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E9%A1%B5%E9%80%9F%E8%A7%88%EF%BC%88%E6%95%B0%E5%AD%97%E6%80%BB%E8%A1%A8%EF%BC%89"><span class="toc-number">3.7.6.</span> <span class="toc-text">一页速览（数字总表）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Referring-Object-Detection-Settings"><span class="toc-number">3.8.</span> <span class="toc-text">Referring Object Detection Settings</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E7%A0%94%E7%A9%B6%E8%83%8C%E6%99%AF%E4%B8%8E%E5%8A%A8%E6%9C%BA"><span class="toc-number">3.8.1.</span> <span class="toc-text">一、研究背景与动机</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%AE%9E%E9%AA%8C%E8%AE%BE%E7%BD%AE%E4%B8%8E%E5%AF%B9%E6%AF%94%E6%96%B9%E6%B3%95"><span class="toc-number">3.8.2.</span> <span class="toc-text">二、实验设置与对比方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E4%B8%BB%E8%A6%81%E7%BB%93%E6%9E%9C%EF%BC%88%E8%A1%A8-5-%E6%91%98%E8%A6%81%EF%BC%89"><span class="toc-number">3.8.3.</span> <span class="toc-text">三、主要结果（表 5 摘要）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E4%BD%9C%E8%80%85%E5%88%86%E6%9E%90%E4%B8%8E%E7%BB%93%E8%AE%BA"><span class="toc-number">3.8.4.</span> <span class="toc-text">四、作者分析与结论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E5%BB%B6%E4%BC%B8%E8%AE%A8%E8%AE%BA"><span class="toc-number">3.8.5.</span> <span class="toc-text">五、延伸讨论</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Effects-of-RefC-and-COCO-Data"><span class="toc-number">3.9.</span> <span class="toc-text">Effects of RefC and COCO Data</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E7%A0%94%E7%A9%B6%E5%8A%A8%E6%9C%BA%E4%B8%8E%E7%9B%AE%E6%A0%87"><span class="toc-number">3.9.1.</span> <span class="toc-text">一、研究动机与目标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%AE%9E%E9%AA%8C%E8%AE%BE%E7%BD%AE"><span class="toc-number">3.9.2.</span> <span class="toc-text">二、实验设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90"><span class="toc-number">3.9.3.</span> <span class="toc-text">三、结果分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E4%BD%9C%E8%80%85%E6%80%BB%E7%BB%93%E4%B8%8E%E6%B4%9E%E5%AF%9F"><span class="toc-number">3.9.4.</span> <span class="toc-text">四、作者总结与洞察</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E5%8F%AF%E8%A7%86%E5%8C%96%E7%90%86%E8%A7%A3"><span class="toc-number">3.9.5.</span> <span class="toc-text">五、可视化理解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%9C%85-%E6%80%BB%E7%BB%93%E4%B8%80%E5%8F%A5%E8%AF%9D"><span class="toc-number">3.9.6.</span> <span class="toc-text">✅ 总结一句话</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Ablations"><span class="toc-number">3.10.</span> <span class="toc-text">Ablations</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E7%A0%94%E7%A9%B6%E7%9B%AE%E7%9A%84"><span class="toc-number">3.10.1.</span> <span class="toc-text">一、研究目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%AE%9E%E9%AA%8C%E8%AE%BE%E7%BD%AE-1"><span class="toc-number">3.10.2.</span> <span class="toc-text">二、实验设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E7%BB%93%E6%9E%9C%E4%B8%8E%E5%88%86%E6%9E%90"><span class="toc-number">3.10.3.</span> <span class="toc-text">三、结果与分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E4%BD%9C%E8%80%85%E8%BF%9B%E4%B8%80%E6%AD%A5%E8%AE%A8%E8%AE%BA"><span class="toc-number">3.10.4.</span> <span class="toc-text">四、作者进一步讨论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E6%80%BB%E4%BD%93%E7%BB%93%E8%AE%BA%EF%BC%88%E4%BD%9C%E8%80%85%E5%8E%9F%E6%96%87%E6%80%BB%E7%BB%93%EF%BC%89"><span class="toc-number">3.10.5.</span> <span class="toc-text">五、总体结论（作者原文总结）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%9C%85-%E5%B0%8F%E7%BB%93%E4%B8%80%E5%8F%A5%E8%AF%9D"><span class="toc-number">3.10.6.</span> <span class="toc-text">✅ 小结一句话</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Conclusion"><span class="toc-number">4.</span> <span class="toc-text">Conclusion</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E6%80%BB%E4%BD%93%E5%AE%9A%E4%BD%8D%EF%BC%9A%E8%AE%A9-DINO-%E6%8B%A5%E6%8A%B1%E5%BC%80%E6%94%BE%E4%B8%96%E7%95%8C"><span class="toc-number">4.1.</span> <span class="toc-text">一、总体定位：让 DINO 拥抱开放世界</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E4%B8%BB%E8%A6%81%E5%88%9B%E6%96%B0%E7%82%B9%E6%80%BB%E7%BB%93"><span class="toc-number">4.2.</span> <span class="toc-text">二、主要创新点总结</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%EF%B8%8F%E2%83%A3-Tight-Fusion-%E8%B7%A8%E6%A8%A1%E6%80%81%E8%9E%8D%E5%90%88%E6%9E%B6%E6%9E%84"><span class="toc-number">4.2.1.</span> <span class="toc-text">1️⃣ Tight Fusion 跨模态融合架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%EF%B8%8F%E2%83%A3-Sub-sentence-Level-Text-Representation"><span class="toc-number">4.2.2.</span> <span class="toc-text">2️⃣ Sub-sentence Level Text Representation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%EF%B8%8F%E2%83%A3-Open-set-Extension-to-Referring-Expression-Comprehension-REC"><span class="toc-number">4.2.3.</span> <span class="toc-text">3️⃣ Open-set Extension to Referring Expression Comprehension (REC)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E5%85%B3%E9%94%AE%E5%AE%9E%E9%AA%8C%E7%BB%93%E8%AE%BA"><span class="toc-number">4.3.</span> <span class="toc-text">三、关键实验结论</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E6%95%B4%E4%BD%93%E8%B4%A1%E7%8C%AE%E4%B8%8E%E6%84%8F%E4%B9%89"><span class="toc-number">4.4.</span> <span class="toc-text">四、整体贡献与意义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B%EF%BC%88%E9%9A%90%E5%90%AB%E5%9C%A8%E7%BB%93%E8%AE%BA%E4%B8%AD%E7%9A%84%E5%90%AF%E7%A4%BA%EF%BC%89"><span class="toc-number">4.5.</span> <span class="toc-text">五、未来展望（隐含在结论中的启示）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%9C%85-%E6%80%BB%E7%BB%93%E4%B8%80%E5%8F%A5%E8%AF%9D-1"><span class="toc-number">4.6.</span> <span class="toc-text">✅ 总结一句话</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/blog/2025/10/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AGrounding-DINO-Marrying-DINO-with-Grounded-Pre-Training-for-Open-Set-Object-Detection/" title="论文阅读：Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection"><img src="/blog/image/A1-1.jpg" onerror="this.onerror=null;this.src='/blog/img/404.jpg'" alt="论文阅读：Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection"/></a><div class="content"><a class="title" href="/blog/2025/10/23/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AGrounding-DINO-Marrying-DINO-with-Grounded-Pre-Training-for-Open-Set-Object-Detection/" title="论文阅读：Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection">论文阅读：Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection</a><time datetime="2025-10-23T01:48:44.000Z" title="发表于 2025-10-23 09:48:44">2025-10-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/blog/2025/10/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AZero-Shot-Detection/" title="论文阅读：Zero-Shot Detection"><img src="/blog/image/62aa03063f117eaad7c77592e3b98d7f05b0a86329e44a-TuUO3E.jpg" onerror="this.onerror=null;this.src='/blog/img/404.jpg'" alt="论文阅读：Zero-Shot Detection"/></a><div class="content"><a class="title" href="/blog/2025/10/20/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AZero-Shot-Detection/" title="论文阅读：Zero-Shot Detection">论文阅读：Zero-Shot Detection</a><time datetime="2025-10-20T11:34:17.000Z" title="发表于 2025-10-20 19:34:17">2025-10-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/blog/2025/10/17/Zero-Shot-Object-Detection/" title="Zero-Shot Object Detection"><img src="/blog/image/5268d877a2a04864b36b4961ab793f4f.jpg" onerror="this.onerror=null;this.src='/blog/img/404.jpg'" alt="Zero-Shot Object Detection"/></a><div class="content"><a class="title" href="/blog/2025/10/17/Zero-Shot-Object-Detection/" title="Zero-Shot Object Detection">Zero-Shot Object Detection</a><time datetime="2025-10-17T10:22:18.000Z" title="发表于 2025-10-17 18:22:18">2025-10-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/blog/2025/10/17/DeViSE-A-Deep-Visual-Semantic-Embedding-Model/" title="DeViSE: A Deep Visual-Semantic Embedding Model"><img src="/blog/image/2aa2662f-d453-4a09-8890-87440bd087b8.png" onerror="this.onerror=null;this.src='/blog/img/404.jpg'" alt="DeViSE: A Deep Visual-Semantic Embedding Model"/></a><div class="content"><a class="title" href="/blog/2025/10/17/DeViSE-A-Deep-Visual-Semantic-Embedding-Model/" title="DeViSE: A Deep Visual-Semantic Embedding Model">DeViSE: A Deep Visual-Semantic Embedding Model</a><time datetime="2025-10-17T06:20:10.000Z" title="发表于 2025-10-17 14:20:10">2025-10-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/blog/2025/09/23/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" title="小样本目标检测"><img src="/blog/image/29051e0d6c0e82fe7e46a7e50399ff577917a1a3c82ee-6Qx5wo.jpg" onerror="this.onerror=null;this.src='/blog/img/404.jpg'" alt="小样本目标检测"/></a><div class="content"><a class="title" href="/blog/2025/09/23/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" title="小样本目标检测">小样本目标检测</a><time datetime="2025-09-23T08:06:54.000Z" title="发表于 2025-09-23 16:06:54">2025-09-23</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2025 By lian</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">岁岁平，岁岁安，岁岁平安</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/blog/js/utils.js"></script><script src="/blog/js/main.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script async data-pjax src="{&quot;site_uv&quot;:true,&quot;site_pv&quot;:true,&quot;page_pv&quot;:true}"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/blog/js/search/local-search.js"></script></div></div></body></html>