<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>A Review of Optical and SAR Image Deep Feature Fusion in Semantic Segmentation | 且离且安的碎碎念</title><meta name="author" content="lian"><meta name="copyright" content="lian"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="好的，同学们，请坐。今天我们要一起深入探讨一篇非常有价值的综述性文献：《A Review of Optical and SAR Image Deep Feature Fusion in Semantic Segmentation》，即《光学与SAR影像在语义分割中的深度特征融合综述》。 我知道大家可能对遥感、合成孔径雷达（SAR）或者深度学习这些领域不甚了解，但这没关系。我会像上课一样，一步步地为">
<meta property="og:type" content="article">
<meta property="og:title" content="A Review of Optical and SAR Image Deep Feature Fusion in Semantic Segmentation">
<meta property="og:url" content="https://qieliqiean.github.io/blog/2025/08/27/A-Review-of-Optical-and-SAR-Image-Deep-Feature-Fusion-in-Semantic-Segmentation/index.html">
<meta property="og:site_name" content="且离且安的碎碎念">
<meta property="og:description" content="好的，同学们，请坐。今天我们要一起深入探讨一篇非常有价值的综述性文献：《A Review of Optical and SAR Image Deep Feature Fusion in Semantic Segmentation》，即《光学与SAR影像在语义分割中的深度特征融合综述》。 我知道大家可能对遥感、合成孔径雷达（SAR）或者深度学习这些领域不甚了解，但这没关系。我会像上课一样，一步步地为">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://qieliqiean.github.io/blog/image/IMG_20250131_155849.jpg">
<meta property="article:published_time" content="2025-08-27T08:57:04.000Z">
<meta property="article:modified_time" content="2026-01-17T09:56:14.937Z">
<meta property="article:author" content="lian">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qieliqiean.github.io/blog/image/IMG_20250131_155849.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "A Review of Optical and SAR Image Deep Feature Fusion in Semantic Segmentation",
  "url": "https://qieliqiean.github.io/blog/2025/08/27/A-Review-of-Optical-and-SAR-Image-Deep-Feature-Fusion-in-Semantic-Segmentation/",
  "image": "https://qieliqiean.github.io/blog/image/IMG_20250131_155849.jpg",
  "datePublished": "2025-08-27T08:57:04.000Z",
  "dateModified": "2026-01-17T09:56:14.937Z",
  "author": [
    {
      "@type": "Person",
      "name": "lian",
      "url": "https://qieliqiean.github.io/blog/"
    }
  ]
}</script><link rel="shortcut icon" href="/blog/image/1.jpg"><link rel="canonical" href="https://qieliqiean.github.io/blog/2025/08/27/A-Review-of-Optical-and-SAR-Image-Deep-Feature-Fusion-in-Semantic-Segmentation/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="stylesheet" href="/blog/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6b5d1303d19816191830cd73eccfdb1e";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
btf.addGlobalFn('pjaxComplete', () => {
  _hmt.push(['_trackPageview',window.location.pathname])
}, 'baidu_analytics')
</script><script>const GLOBAL_CONFIG = {
  root: '/blog/',
  algolia: undefined,
  localSearch: {"path":"/blog/search.xml","preload":true,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":230,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'A Review of Optical and SAR Image Deep Feature Fusion in Semantic Segmentation',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/blog/image/IMG_20250131_155849.jpg" onerror="this.onerror=null;this.src='/blog/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/blog/archives/"><div class="headline">文章</div><div class="length-num">42</div></a><a href="/blog/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/blog/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/blog/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/blog/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/blog/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/blog/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/blog/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/blog/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/blog/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(image/1363709.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/blog/"><img class="site-icon" src="/blog/image/background1.png" alt="Logo"><span class="site-name">且离且安的碎碎念</span></a><a class="nav-page-title" href="/blog/"><span class="site-name">A Review of Optical and SAR Image Deep Feature Fusion in Semantic Segmentation</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/blog/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/blog/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/blog/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/blog/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/blog/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/blog/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/blog/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">A Review of Optical and SAR Image Deep Feature Fusion in Semantic Segmentation</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-08-27T08:57:04.000Z" title="发表于 2025-08-27 16:57:04">2025-08-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2026-01-17T09:56:14.937Z" title="更新于 2026-01-17 17:56:14">2026-01-17</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">4.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>14分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>好的，同学们，请坐。今天我们要一起深入探讨一篇非常有价值的综述性文献：《A Review of Optical and SAR Image Deep Feature Fusion in Semantic Segmentation》，即《光学与SAR影像在语义分割中的深度特征融合综述》。</p>
<p>我知道大家可能对遥感、合成孔径雷达（SAR）或者深度学习这些领域不甚了解，但这没关系。我会像上课一样，一步步地为大家拆解这篇文章的每一个概念、每一个细节。请随时在脑海里记下问题，我们争取在讲解结束时，让大家对这个领域有一个清晰、全面的认识。</p>
<hr>
<h3 id="第一部分：文献概述与核心概念解读-引言-Introduction"><strong>第一部分：文献概述与核心概念解读 (引言 Introduction)</strong></h3>
<p>首先，我们来看标题。这篇论文的标题就包含了它的核心内容，我们来逐个剖析：</p>
<ol>
<li>
<p><strong>语义分割 (Semantic Segmentation)</strong>：这是我们的<strong>目标任务</strong>。想象一下你有一张航拍照片，语义分割就是要给这张照片里的<strong>每一个像素</strong>都打上一个标签。比如，这个像素是“建筑”，那个像素是“水体”，旁边的像素是“植被”。它不像目标检测那样只画一个框框住一栋楼，而是要精确到像素级别，把楼的轮廓完整地描绘出来。这在城市规划、土地资源管理、灾害监测等领域至关重要。你看论文中的 <strong>图1 (Fig. 1)</strong>，就展示了语义分割的几个主要应用方向，其中土地利用/土地覆盖（LULC）分类占了半壁江山。</p>
</li>
<li>
<p><strong>光学影像 (Optical Images)</strong>：这是我们最熟悉的<strong>数据源之一</strong>。它就像我们手机或数码相机拍出的照片，记录的是地物反射的可见光和红外光。它的<strong>优点</strong>是信息丰富、色彩鲜艳、纹理清晰，非常直观。但<strong>缺点</strong>也很致命：它依赖太阳光，所以晚上无法成像；更重要的是，它穿不透云、雾、霾，天气不好就“抓瞎”。</p>
</li>
<li>
<p><strong>SAR影像 (Synthetic Aperture Radar Images)</strong>：这是另一种关键的<strong>数据源</strong>。SAR是一种<strong>主动式</strong>的微波雷达。所谓“主动”，就是它自己发射电磁波，然后接收地物反射回来的回波来成像，就像蝙蝠的声纳系统。这赋予了它两大<strong>优点</strong>：</p>
<ul>
<li><strong>全天时全天候</strong>：不依赖太阳光，白天黑夜都能工作；微波能穿透云雾，不受天气影响。</li>
<li><strong>对结构敏感</strong>：它对地物的几何结构、材质、粗糙度非常敏感。比如，金属建筑的回波会非常强。
它的<strong>缺点</strong>是图像不直观，看起来是灰度的，而且充满了“相干斑噪声”（speckle noise），就像电视雪花点一样，信噪比低。</li>
</ul>
<p>大家可以看 <strong>图2 (Fig. 2)</strong>，左边是光学影像，右边是同一地点的SAR影像。能明显看出它们的成像机理和视觉表现完全不同。</p>
</li>
<li>
<p><strong>深度特征融合 (Deep Feature Fusion)</strong>：这是本文探讨的<strong>核心技术</strong>。既然光学和SAR影像各有优劣，那么把它们的信息结合起来，不就能取长补短，得到更准确的结果吗？这就是“融合”的目的。而“深度特征”指的是我们不直接融合原始的像素，而是利用<strong>深度学习</strong>（特别是卷积神经网络CNN）来分别从两种影像中提取出更高级、更抽象的特征信息（比如建筑的边缘、植被的纹理），然后再对这些“特征”进行融合。</p>
</li>
</ol>
<p>所以，这篇论文要解决的核心问题是：<strong>如何最有效地利用深度学习技术，将光学影像和SAR影像的优势特征结合起来，以实现更精准的遥感影像语义分割。</strong></p>
<p>这篇论文是一篇<strong>综述 (Review)</strong>，意味着它的目的不是提出一个全新的方法，而是系统地梳理、总结和评价该领域现有的研究成果，指出当前的挑战，并展望未来的发展方向。这对于我们初学者来说，是进入一个领域的绝佳向导。</p>
<hr>
<h3 id="第二部分：遥感影像语义分割的方法演进-Section-II"><strong>第二部分：遥感影像语义分割的方法演进 (Section II)</strong></h3>
<p>在讨论“融合”之前，我们得先了解“分割”本身是怎么做的。作者首先回顾了遥感影像语义分割技术的发展。</p>
<ol>
<li>
<p><strong>传统方法</strong>：在深度学习兴起之前，研究者们需要手动设计特征，比如纹理特征、光谱特征、形状特征等，然后用支持向量机（SVM）、随机森林等传统机器学习方法进行分类。这种方法非常依赖专家的经验，耗时耗力，而且设计的特征泛化能力差。</p>
</li>
<li>
<p><strong>基于深度学习的方法</strong>：随着数据量的增大和计算能力的提升，深度学习成为了主流。文章介绍了几个关键的架构：</p>
<ul>
<li><strong>CNN (卷积神经网络)</strong>：早期的应用是采用“滑窗法”（见 <strong>图6</strong>）。就是用一个小窗口在图像上滑动，每次对窗口内的图像块进行分类，判断中心像素属于哪一类。这种方法效率低下，且会丢失像素间的空间关联信息。</li>
<li><strong>FCN (全卷积网络)</strong>：这是一个里程碑式的进步（见 <strong>图7</strong>）。FCN将传统CNN最后的“全连接层”换成了“卷积层”，实现了从输入一张完整图像到输出一张同样大小的分割图的“端到端”处理。它经典的“编码器-解码器”（Encoder-Decoder）结构，先通过编码器（卷积和池化）不断压缩图像，提取高级语义特征；再通过解码器（上采样和反卷积）逐步恢复图像尺寸，实现像素级的定位。像U-Net、DeepLab都是这个思想的优秀代表。</li>
<li><strong>RNN (循环神经网络)</strong>：RNN擅长处理序列数据。在遥感领域，这通常指<strong>时间序列</strong>影像，比如对同一个地方连续拍摄一年的影像来监测作物生长周期。RNN（特别是其变种LSTM）能有效地捕捉这种时间维度上的变化信息（见 <strong>图8</strong>）。</li>
<li><strong>其他新模型（Transformer, Mamba等）</strong>：近年来，源于自然语言处理领域的Transformer模型也被引入了计算机视觉。它通过“自注意力机制”能更好地捕捉图像中的长距离依赖关系。Mamba则是更新的架构，试图在保持长距离建模能力的同时降低计算复杂度。这些模型虽然强大，但它们本身并不能解决单一数据源（比如只有光学影像）的固有缺陷。</li>
</ul>
</li>
</ol>
<p><strong>小结</strong>：这一部分告诉我们，深度学习方法，特别是FCN架构，是当前语义分割的主流。但无论模型设计得多么精巧，如果只用单一类型的数据，总会遇到瓶颈。比如，光学影像会因为云雾或阴影导致分割错误，而SAR影像则可能因为噪声干扰而边界模糊。这就自然而然地引出了“融合”的必要性。</p>
<hr>
<h3 id="第三部分：影像融合的三种策略-Section-III"><strong>第三部分：影像融合的三种策略 (Section III)</strong></h3>
<p>好，既然要融合，具体该怎么做呢？作者总结了三种不同层次的融合策略。大家可以参考 <strong>表一 (Table I)</strong> 和 <strong>图9、10、11</strong>。</p>
<ol>
<li>
<p><strong>像素级融合 (Pixel-level Fusion / Early Fusion)</strong>：这是最直接的融合方式（见 <strong>图9</strong>）。在输入网络<strong>之前</strong>，就将配准好的光学和SAR影像在像素层面进行合并，比如将SAR影像作为一个额外的通道疊加到光学影像上，然后一起送入一个深度学习网络。</p>
<ul>
<li><strong>优点</strong>：最大限度地保留了原始信息。</li>
<li><strong>缺点</strong>：对影像的配准精度要求极高；两种影像的物理意义和数据分布差异巨大，强行“捆绑”在一起，可能会让网络难以学习，甚至相互干扰。</li>
</ul>
</li>
<li>
<p><strong>决策级融合 (Decision-level Fusion / Late Fusion)</strong>：这是最末端的融合（见 <strong>图10</strong>）。我们分别用两个独立的网络处理光学和SAR影像，各自得到一个初步的分割结果（决策），最后再用某种规则（如投票、加权平均等）将这两个结果融合，得到最终的分割图。</p>
<ul>
<li><strong>优点</strong>：对配准要求低，实现简单，容错性好。</li>
<li><strong>缺点</strong>：信息损失严重。因为两个网络在中间过程完全没有交流，很多有价值的中间特征都被丢弃了，只融合了最终的“粗糙”结果，无法实现深层次的互补。</li>
</ul>
</li>
<li>
<p><strong>特征级融合 (Feature-level Fusion / Intermediate Fusion)</strong>：这是目前研究的<strong>重点和主流</strong>（见 <strong>图11</strong>）。采用双分支（或多分支）网络结构，一个分支处理光学影像，另一个处理SAR影像。网络在中间的某几层将两个分支提取出的<strong>特征图 (feature maps)</strong> 进行融合，然后继续后续的处理。</p>
<ul>
<li><strong>优点</strong>：这是像素级和决策级融合的折中。它既避免了在原始数据层面的直接冲突，又能在抽象的特征层面进行深度交互，让网络学习到两种数据如何互补。</li>
<li><strong>缺点</strong>：融合模块的设计更复杂，需要仔细考虑在哪个阶段融合、以及如何融合。</li>
</ul>
</li>
</ol>
<p>这篇文章的<strong>核心</strong>，正是围绕着第三种策略——<strong>特征级融合</strong>展开的。</p>
<hr>
<h3 id="第四部分：光学与SAR特征级融合的核心技术-Section-IV"><strong>第四部分：光学与SAR特征级融合的核心技术 (Section IV)</strong></h3>
<p>这是本文最精华的部分。作者将特征级融合的方法，根据其融合模块的设计思路，分成了四大类。我们可以结合 <strong>表二 (Table II)</strong> 和后面的大量图示来理解。</p>
<p>在介绍这些策略前，要明确一个前提：大部分方法都采用<strong>双分支网络</strong>，即一个分支提光学特征，一个分支提SAR特征，然后在中间用下面这些“花式”模块进行融合。</p>
<ol>
<li>
<p><strong>线性融合策略 (Linear Fusion Strategy)</strong>：这是最基础的融合方法。</p>
<ul>
<li><strong>特征拼接 (Concatenation)</strong>：见 <strong>图12</strong>。将两个分支的特征图在通道维度上直接堆叠起来。比如，光学特征图有64个通道，SAR特征图也有64个通道，拼接后就得到一个128通道的新特征图。</li>
<li><strong>特征求和 (Summation)</strong>：见 <strong>图13</strong>。将两个特征图对应元素相加。这要求它们的通道数相同。</li>
<li><strong>特征点积 (Dot Product)</strong>：见 <strong>图14</strong>。对应元素相乘。
<strong>评价</strong>：简单粗暴，易于实现。但缺点是它平等地对待所有特征，没有区分哪些特征更重要，也无法进行复杂的信息交互。</li>
</ul>
</li>
<li>
<p><strong>基于注意力的融合策略 (Attention-based Fusion Strategy)</strong>：这是目前非常流行的方法。其核心思想是模仿人类的视觉注意力机制，让网络<strong>学会关注重要的信息，抑制无关的信息</strong>。</p>
<ul>
<li><strong>通道注意力 (Channel Attention)</strong>：见 <strong>图15</strong>。它学习每个特征<strong>通道</strong>的重要性。比如，对于识别植被，光学影像的“近红外”通道可能比SAR的某个通道更重要，那么网络就给“近红外”通道分配更高的权重。</li>
<li><strong>空间注意力 (Spatial Attention)</strong>：见 <strong>图16</strong>。它学习特征图上<strong>不同空间位置</strong>的重要性。比如，在分割建筑时，网络应该更关注那些有明显边缘轮廓的区域。</li>
<li><strong>混合注意力 (Mixed Attention)</strong>：见 <strong>图17</strong>。将通道和空间注意力结合起来，效果更好。</li>
<li><strong>自注意力 (Self-Attention)</strong>：见 <strong>图21</strong>。不仅关注局部，还能建立图像中任意两个像素之间的关系，捕捉长距离依赖。
<strong>评价</strong>：注意力机制能让融合过程变得更“智能”，动态地调整不同模态、不同特征的权重，极大地提升了融合效果。</li>
</ul>
</li>
<li>
<p><strong>基于门控的融合策略 (Gate-based Fusion Strategy)</strong>：这个策略可以理解为设置一个“信息阀门”或“门卫”。网络学习一个“门控单元”（gate），来控制来自不同分支的信息流。</p>
<ul>
<li><strong>独立门 (Independent Gate)</strong>：见 <strong>图22</strong>。为光学和SAR特征分别设置独立的门，各自控制流量。</li>
<li><strong>互补门 (Complementary Gate)</strong>：见 <strong>图23, 25, 26</strong>。只学习一个门控信号G（值在0-1之间）。光学特征乘以G，SAR特征则乘以(1-G)。这样一来，两者就有了此消彼长的互补关系。</li>
<li><strong>交互门 (Interactive Gate)</strong>：见 <strong>图24</strong>。这是最复杂的。光学特征的“阀门”由SAR特征来决定，反之亦然。实现了深度的信息交互。
<strong>评价</strong>：门控机制能非常灵活地筛选和加权特征，过滤掉冗余或噪声信息，保留最有用的互补特征。</li>
</ul>
</li>
<li>
<p><strong>特征对齐策略 (Feature Alignment Strategy)</strong>：这个策略直面一个根本问题：光学和SAR的成像机理差异太大，导致它们提取出的特征分布在完全不同的“特征空间”里，好比一个说中文，一个说英文，直接融合会“鸡同鸭讲”。</p>
<ul>
<li><strong>思路</strong>：在融合之前，先通过某种变换，将两种特征“翻译”到一个<strong>共同的、对齐的特征空间</strong>中（见 <strong>图27</strong>），让它们的语义能够对应上，然后再进行融合。
<strong>评价</strong>：这个策略从根本上解决了异质性（heterogeneity）带来的问题，能有效提升分割精度，尤其是在边界和细节上。</li>
</ul>
</li>
</ol>
<p><strong>小结</strong>：第四部分是技术的“军火库”。从简单的线性叠加，到智能的注意力加权，再到精细的门控筛选和根本的特征对齐，研究者们设计了各种巧妙的模块来解决特征融合的难题。在实际应用中，这些策略也常常被组合使用。</p>
<hr>
<h3 id="第五、六部分：实践基础——数据集与评价指标"><strong>第五、六部分：实践基础——数据集与评价指标</strong></h3>
<ol>
<li>
<p><strong>数据集 (Datasets, Section V)</strong>：工欲善其事，必先利其器。深度学习是数据驱动的，没有好的数据集，再好的模型也只是空中楼阁。作者在 <strong>表三 (Table III)</strong> 中为我们整理了当前公开可用的、同时包含光学和SAR影像的语义分割数据集。</p>
<ul>
<li><strong>核心痛点</strong>：作者指出，目前该领域<strong>严重缺乏大规模、高分辨率、像素级精准配准</strong>的基准数据集。这是制约领域发展的一大瓶颈。现有数据集要么分辨率不够高，要么覆盖范围小，要么类别不够丰富。</li>
</ul>
</li>
<li>
<p><strong>评价指标 (Evaluation Indicators, Section VI)</strong>：我们怎么评价一个分割模型的好坏呢？作者介绍了几个常用的指标：</p>
<ul>
<li><strong>PA (Pixel Accuracy)</strong>：像素精度。就是被正确分类的像素占总像素的比例。简单直观，但当类别不均衡时（比如背景像素远多于目标像素），这个指标会产生误导。</li>
<li><strong>MPA (Mean Pixel Accuracy)</strong>：平均像素精度。计算每个类别的像素精度，然后取平均。比PA更公平。</li>
<li><strong>IoU (Intersection over Union)</strong>：交并比。这是语义分割中<strong>最核心、最常用</strong>的指标。它衡量的是预测结果与真实标签之间的重合程度，计算公式是 <code>(预测区域 ∩ 真实区域) / (预测区域 ∪ 真实区域)</code>。值越接近1，说明分割得越好。</li>
<li><strong>MIoU (Mean IoU)</strong>：平均交并比。计算每个类别的IoU，然后取平均。这是评价模型整体性能的黄金标准。</li>
<li><strong>计算复杂度</strong>：除了精度，模型的运行速度（FLOPs）和内存占用（参数量）也是实际应用中需要考虑的重要因素。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="第七、八部分：挑战、未来方向与总结"><strong>第七、八部分：挑战、未来方向与总结</strong></h3>
<p>最后，作为一篇优秀的综述，作者高屋建瓴地指出了当前面临的挑战和未来的研究方向。</p>
<ol>
<li>
<p><strong>主要挑战 (Challenges)</strong>：</p>
<ul>
<li><strong>特征贡献度分配难题</strong>：对于不同的地物类别，光学和SAR的贡献度应该是不同的。比如，识别植被，光学数据更重要；而识别建筑轮廓，SAR数据可能更有用。如何让模型<strong>动态地、自适应地</strong>为不同类别分配不同模态的权重，是一个核心挑战。</li>
<li><strong>数据需求挑战</strong>：前面提到的，缺乏高质量的基准数据集。这不仅阻碍了新算法的公平比较，也限制了模型的性能上限。</li>
</ul>
</li>
<li>
<p><strong>未来方向 (Future Directions)</strong>：</p>
<ul>
<li><strong>动态特征贡献度分配</strong>：设计更智能的网络，能学习到模态与类别之间的复杂关系。</li>
<li><strong>构建大规模基准数据集</strong>：这是整个社区的共同呼声，需要有组织地去采集、标注和发布。</li>
<li><strong>小样本/低质量数据下的融合</strong>：在现实中，我们可能只有少量标注数据或质量不佳的数据，如何在这种情况下做好融合，是一个很有价值的研究方向（例如，利用无监督或自监督学习）。</li>
<li><strong>基于大模型的融合研究</strong>：随着“基础模型”（Foundation Model）的兴起，如何利用这些预训练好的大模型来增强多模态信息的融合能力，是一个前沿热点。</li>
</ul>
</li>
<li>
<p><strong>总结 (Conclusion)</strong>：文章最后总结，深度学习极大地推动了遥感语义分割的发展。光学与SAR影像的互补性使其融合成为研究热点。本文系统地回顾了从分割方法到融合策略的技术演进，特别是详细梳理了各种特征级融合模块的设计思路，为研究者和从业者提供了一个全面的技术概览和清晰的未来路线图。</p>
</li>
</ol>
<hr>
<p>好了，同学们，以上就是我对这篇综述文献的详细讲解。希望通过这次“课程”，大家不仅理解了这篇论文的内容，也对“光学与SAR影像融合”这个研究领域建立起了一个宏观的认知框架。我们看到了这个领域的巨大潜力和价值，也认识到了它所面临的挑战。这正是一个充满机遇的研究方向。</p>
<p>大家有什么问题吗？</p>
<hr>
<h1 id="专业术语">专业术语</h1>
<ul>
<li><strong>语义分割（Semantic Segmentation）</strong>：计算机视觉与遥感领域的核心任务之一，核心是为图像中的每个像素分配对应的语义类别标签（如 “农田”“建筑”“水体” 等），实现从 “图像” 到 “语义信息” 的转化。</li>
<li><strong>配准 (Co-registration)</strong> 指的是将来自不同传感器、在不同时间拍摄的关于同一区域的图像，进行几何上的对齐，使得图像中同一个地物在所有图像上的像素坐标都是一致的。</li>
</ul>
</article><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/blog/image/IMG_20250131_155849.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/blog/2025/08/07/%E8%87%AA%E7%84%B6%E8%BE%A9%E8%AF%81%E6%B3%95/" title="自然辩证法"><img class="cover" src="/blog/image/d4f0a9ce880411ebb6edd017c2d2eca2.png" onerror="onerror=null;src='/blog/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">自然辩证法</div></div><div class="info-2"><div class="info-item-1"> 辩证唯物主义自然观的主要观点和特征？  定义：关于自然界及其与人类关系的总观点 主要观点：自然界是客观的、变化发展的物质世界；物质在其永恒的循环中按照规律运动；物质运动在量和质的方面都是不灭的，时间和空间是物质的固有属性和存在方式；人是自然界的一部分，意识和思维是人脑的机能；实践是人类有目的地认识和改造自然界的能动活动，是人类存在的本质和方式；人是自然界的一部分，“人靠自然界生活”，“人与自然是生命共同体”；“人与自然是一种共生关系，对自然的伤害最终会伤及人类自身”。 认识自然界要遵循客观性原则。 特征： ①实践性：主张自然界是人类社会实践的产物，实践对认识自然界起到决定作用 ②历史性：主张自然界的历史是人类生成的历史和自然界对人的生成的历史，认识自然界也是以实践为基础的过程 ③辩证性：以实践论为基础，实现唯物论和辩证法的统一、天然自然界和人工自然界的统一、人类史和自然史的统一、人与自然界关系上的能动性和受动性的统一 ④批判性：辩证唯物主义自然观批判众多旧观念和错误思想，引入自觉辩证法用于自然观和历史观...</div></div></div></a><a class="pagination-related" href="/blog/2025/08/31/%E8%AE%BA%E6%96%87-%E5%B0%8F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%9ATowards-Large-Scale-Small-Object-Detection-Survey-and-Benchmarks/" title="论文-小目标检测：Towards Large-Scale Small Object Detection:  Survey and Benchmarks"><img class="cover" src="/blog/image/Date%EF%BC%9A20250817154338.jpg" onerror="onerror=null;src='/blog/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">论文-小目标检测：Towards Large-Scale Small Object Detection:  Survey and Benchmarks</div></div><div class="info-2"><div class="info-item-1">Towards Large-Scale Small Object Detection: Survey and Benchmarks 一、文章基础信息   期刊 / 年份：IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (TPAMI)，2023 年 11 月   核心主题：小目标检测（Small Object Detection, SOD）的系统综述、两大专用大规模基准数据集（SODA）构建、主流算法评估   作者单位：西北工业大学自动化学院   开源资源：数据集与代码地址：https://shaunyuan22.github.io/SODA；补充材料地址：https://doi.org/10.1109/TPAMI.2023.3290594   二、摘要（核心浓缩） 1. 领域背景   深度卷积神经网络（DCNN）推动目标检测显著进步，但小目标检测（SOD）仍是计算机视觉公认难点—— 核心原因：小目标固有结构导致 “视觉外观差（细节模糊）” 和...</div></div></div></a></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/blog/image/IMG_20250131_155849.jpg" onerror="this.onerror=null;this.src='/blog/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">lian</div><div class="author-info-description">太平山上修真我，祖师堂中续香火</div><div class="site-data"><a href="/blog/archives/"><div class="headline">文章</div><div class="length-num">42</div></a><a href="/blog/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/blog/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="mailto:2895014608@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">QQ-2895014608</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%EF%BC%9A%E6%96%87%E7%8C%AE%E6%A6%82%E8%BF%B0%E4%B8%8E%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E8%A7%A3%E8%AF%BB-%E5%BC%95%E8%A8%80-Introduction"><span class="toc-text">第一部分：文献概述与核心概念解读 (引言 Introduction)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%EF%BC%9A%E9%81%A5%E6%84%9F%E5%BD%B1%E5%83%8F%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%9A%84%E6%96%B9%E6%B3%95%E6%BC%94%E8%BF%9B-Section-II"><span class="toc-text">第二部分：遥感影像语义分割的方法演进 (Section II)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%EF%BC%9A%E5%BD%B1%E5%83%8F%E8%9E%8D%E5%90%88%E7%9A%84%E4%B8%89%E7%A7%8D%E7%AD%96%E7%95%A5-Section-III"><span class="toc-text">第三部分：影像融合的三种策略 (Section III)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%EF%BC%9A%E5%85%89%E5%AD%A6%E4%B8%8ESAR%E7%89%B9%E5%BE%81%E7%BA%A7%E8%9E%8D%E5%90%88%E7%9A%84%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF-Section-IV"><span class="toc-text">第四部分：光学与SAR特征级融合的核心技术 (Section IV)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%94%E3%80%81%E5%85%AD%E9%83%A8%E5%88%86%EF%BC%9A%E5%AE%9E%E8%B7%B5%E5%9F%BA%E7%A1%80%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8E%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="toc-text">第五、六部分：实践基础——数据集与评价指标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%83%E3%80%81%E5%85%AB%E9%83%A8%E5%88%86%EF%BC%9A%E6%8C%91%E6%88%98%E3%80%81%E6%9C%AA%E6%9D%A5%E6%96%B9%E5%90%91%E4%B8%8E%E6%80%BB%E7%BB%93"><span class="toc-text">第七、八部分：挑战、未来方向与总结</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%93%E4%B8%9A%E6%9C%AF%E8%AF%AD"><span class="toc-text">专业术语</span></a></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/blog/2026/01/17/%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB%EF%BC%9Ayolo_world-models-necks-yolo_world_pafpn.py/" title="无标题">无标题</a><time datetime="2026-01-17T08:27:24.706Z" title="发表于 2026-01-17 16:27:24">2026-01-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/blog/2026/01/17/%E4%B8%AD%E8%BD%AC/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91%E6%95%B4%E7%90%86/" title="无标题">无标题</a><time datetime="2026-01-17T08:27:24.611Z" title="发表于 2026-01-17 16:27:24">2026-01-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/blog/2026/01/17/%E5%9B%BE%E7%89%87%E6%B5%8B%E8%AF%95/" title="图片测试">图片测试</a><time datetime="2026-01-17T03:42:17.000Z" title="发表于 2026-01-17 11:42:17">2026-01-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/blog/2026/01/14/%E3%80%90YOLO-UniOW%E3%80%91%E6%8C%87%E6%A0%87%E8%A7%A3%E9%87%8A_gemini/" title="论文阅读：YOLO-UniOW: Efficient Universal Open-World Object Detection-指标解释-gemini"><img src="/blog/image/A1-2.jpg" onerror="this.onerror=null;this.src='/blog/img/404.jpg'" alt="论文阅读：YOLO-UniOW: Efficient Universal Open-World Object Detection-指标解释-gemini"/></a><div class="content"><a class="title" href="/blog/2026/01/14/%E3%80%90YOLO-UniOW%E3%80%91%E6%8C%87%E6%A0%87%E8%A7%A3%E9%87%8A_gemini/" title="论文阅读：YOLO-UniOW: Efficient Universal Open-World Object Detection-指标解释-gemini">论文阅读：YOLO-UniOW: Efficient Universal Open-World Object Detection-指标解释-gemini</a><time datetime="2026-01-14T01:44:34.000Z" title="发表于 2026-01-14 09:44:34">2026-01-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/blog/2026/01/14/%E3%80%90YOLO-UniOW%E3%80%91%E6%8C%87%E6%A0%87%E8%A7%A3%E9%87%8A_gpt/" title="论文阅读：YOLO-UniOW: Efficient Universal Open-World Object Detection-指标解释-gpt"><img src="/blog/image/A1-2.jpg" onerror="this.onerror=null;this.src='/blog/img/404.jpg'" alt="论文阅读：YOLO-UniOW: Efficient Universal Open-World Object Detection-指标解释-gpt"/></a><div class="content"><a class="title" href="/blog/2026/01/14/%E3%80%90YOLO-UniOW%E3%80%91%E6%8C%87%E6%A0%87%E8%A7%A3%E9%87%8A_gpt/" title="论文阅读：YOLO-UniOW: Efficient Universal Open-World Object Detection-指标解释-gpt">论文阅读：YOLO-UniOW: Efficient Universal Open-World Object Detection-指标解释-gpt</a><time datetime="2026-01-14T01:44:34.000Z" title="发表于 2026-01-14 09:44:34">2026-01-14</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2025 - 2026 By lian</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">岁岁平，岁岁安，岁岁平安</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/blog/js/utils.js"></script><script src="/blog/js/main.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const getGiscusTheme = theme => theme === 'dark' ? 'dark' : 'light'

  const createScriptElement = config => {
    const ele = document.createElement('script')
    Object.entries(config).forEach(([key, value]) => {
      ele.setAttribute(key, value)
    })
    return ele
  }

  const loadGiscus = (el = document, key) => {
    const mappingConfig = isShuoshuo
      ? { 'data-mapping': 'specific', 'data-term': key }
      : { 'data-mapping': (option && option['data-mapping']) || 'pathname' }

    const giscusConfig = {
      src: 'https://giscus.app/client.js',
      'data-repo': 'qieliqiean/blog',
      'data-repo-id': 'R_kgDONvpdYw',
      'data-category-id': 'DIC_kwDONvpdY84C1DqB',
      'data-theme': getGiscusTheme(document.documentElement.getAttribute('data-theme')),
      'data-reactions-enabled': '1',
      crossorigin: 'anonymous',
      async: true,
      ...option,
      ...mappingConfig
    }

    const scriptElement = createScriptElement(giscusConfig)

    el.querySelector('#giscus-wrap').appendChild(scriptElement)

    if (isShuoshuo) {
      window.shuoshuoComment.destroyGiscus = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }
  }

  const changeGiscusTheme = theme => {
    const iframe = document.querySelector('#giscus-wrap iframe')
    if (iframe) {
      const message = {
        giscus: {
          setConfig: {
            theme: getGiscusTheme(theme)
          }
        }
      }
      iframe.contentWindow.postMessage(message, 'https://giscus.app')
    }
  }

  btf.addGlobalFn('themeChange', changeGiscusTheme, 'giscus')

  if (isShuoshuo) {
    'Giscus' === 'Giscus'
      ? window.shuoshuoComment = { loadComment: loadGiscus }
      : window.loadOtherComment = loadGiscus
    return
  }

  if ('Giscus' === 'Giscus' || !true) {
    if (true) btf.loadComment(document.getElementById('giscus-wrap'), loadGiscus)
    else loadGiscus()
  } else {
    window.loadOtherComment = loadGiscus
  }
})()</script></div><script async data-pjax src="https://cdn.jsdelivr.net/npm/busuanzi@2.3.0/bsz.pure.mini.min.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/blog/js/search/local-search.js"></script></div></div></body></html>